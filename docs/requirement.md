要件定義書
**Target Device:** Mac M3 (16GB Memory)
**User:** 友人（非エンジニア）

## 1. システム概要
音声ファイルをドラッグ＆ドロップするだけで、**「1行17文字以内」**かつ**「文脈として自然な区切り」**で構成されたSRT（字幕ファイル）を出力する自動化ツール。
従来ツールのような機械的な分割を排除し、プロの編集者が手動で切ったような「意味のまとまり」を再現することを最重要視する。

## 2. 技術スタック

### 音声認識（3モデルで比較検証）
プロトタイプで3つのモデルを実装。**デフォルトは mlx-whisper large-v3（MLX版）** とする（Plan方針）。

*   **モデル1: mlx-whisper large-v3（汎用×MLX最適化｜デフォルト）**
    *   リポジトリ: `mlx-community/whisper-large-v3-mlx`
    *   Apple Silicon（M3）でMetal GPU加速
    *   word-levelタイムスタンプ対応
    *   **期待値**: 最高精度（専門用語・複雑な内容）

*   **モデル2: kotoba-whisper-v2.0-mlx（日本語特化×MLX最適化）**
    *   リポジトリ: `kaiinui/kotoba-whisper-v2.0-mlx`
    *   distil-large-v3ベース / ReazonSpeech 720万クリップ
    *   **期待値**: 日本語で高速・高精度

*   **モデル3: OpenAI Whisper large-v3（公式実装）**
    *   リポジトリ: `openai/whisper`
    *   MPS（Metal Performance Shaders）対応
    *   word-levelタイムスタンプ対応
    *   **期待値**: 安定性・互換性重視

### 文章整形（LLM API）
以下のプロバイダーから選択可能。**Plan推奨デフォルトは Google (gemini-1.5-flash)**。  
※ CLIでは `--llm` を明示指定しない限り整形とSRT生成は実行されず、文字起こしJSONのみ保存される。  
※ Blocking（事前分割）は無効化し、全文をLLMに渡して意味的改行のみをLLM側で行う。

*   **Google** (gemini-1.5-pro / gemini-1.5-flash) ←推奨
*   **OpenAI** (gpt-4o, gpt-4o-mini など)
*   **Anthropic** (claude-3-5-sonnet-20241022, claude-3-5-haiku-20241022 など) ※MVP完了後に比較テスト予定

**設計方針:**
*   各プロバイダーのAPIキーとモデル名を`.env`ファイルで管理
*   CLIオプション `--llm {openai|google|anthropic}` で実行時に切り替え（未指定なら整形スキップ）
*   非エンジニアでもモデル更新可能（コード変更不要）
*   原文の単語を極力変更せず、アライメント精度を向上

**.env設定例:**
```env
# OpenAI
OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxx
OPENAI_MODEL=gpt-4o-mini
# Google Gemini
GOOGLE_API_KEY=AIzaSyxxxxxxxxxxxxxxxxx
GOOGLE_MODEL=gemini-1.5-flash
# Anthropic Claude
ANTHROPIC_API_KEY=sk-ant-xxxxxxxxxxxxx
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
```

### 言語・環境
*   **Python 3.10-3.12**（3.13は非対応）
*   **開発環境:** Python仮想環境（venv）
*   **Docker不使用の理由:**
    *   MLXはApple SiliconのMetal GPUに直接アクセスする必要がある
    *   DockerはMetal GPUサポートなし（CPU動作になり性能が大幅低下）
    *   ネイティブ環境が最も高速で開発効率も高い

### 依存関係管理
*   **フェーズ1（推奨）:** `pip + venv + requirements.txt`
    *   シンプルで初心者向け
    *   Python標準ツール（追加インストール不要）
    *   このプロジェクト規模では十分
*   **将来的な選択肢:** uv または Poetry
    *   プロジェクトが大規模化した際に検討
    *   バージョン競合の自動解決が必要になったら移行
*   **同一環境に3モデルを共存:**
    *   mlx-whisper（kotoba含む）
    *   openai-whisper（公式）
    *   ※ faster-whisper は現時点で使用しない
*   競合が発生した場合のみ環境分離を検討

### GUI（フェーズ4で実装）
*   **フェーズ1:** GUIなし（Pythonスクリプトのみ）
*   **フェーズ4:** 簡易GUI（Tkinter または Flet）を想定（Planに合わせて後ろ倒し）
*   **将来的な選択肢:** Tauri + React（モダンなUI）

## 3. 処理フロー（内部ロジック）

### Step 1: 精密文字起こし (Local)
*   mlx-whisper（デフォルト）または kotoba / OpenAI Whisper を使用し、音声からテキストデータを作成。
*   **必須:** `word_timestamps=True`を指定して、単語ごとの開始・終了時刻を取得。
    *   例: `{'word': '設定を', 'start': 10.5, 'end': 11.2}, ...`
*   **出力データ:**
    *   全文テキスト
    *   単語レベルのタイムスタンプリスト（JSON形式で保持）

### Step 2: 文脈理解と整形 (Cloud API)
*   Whisper全文をそのまま選択したLLM API（OpenAI/Google/Anthropic）へ送信し、意味的な改行と整形を行う（Blockingなし）。
*   **プロンプトの役割:**
    1.  フィラー（えー、あー等）の削除。
    2.  **「1行17文字以内」**の制約を適用。
    3.  文法・意味構造解析による**「自然な改行位置」**の決定。
        *   *ルール:* 助詞の後ろ、読点、意味の切れ目を優先。
    4.  **重要:** 原文の単語は極力変更しない（表記ゆれのみ修正可）。
    5.  **出力形式:** 各行の最後に、その行の最後の単語を `[WORD: 単語]` の形式で付記。
    6.  （オプションON時のみ）語尾の微調整・リライト。

#### API出力例
```
設定を開いて[WORD: 開いて]
くださいね[WORD: くださいね]
```

#### エラーハンドリング
*   **リトライ戦略:** API失敗時は指数バックオフ（1s → 3s → 5s）で最大3回リトライ
*   **失敗時の動作:**
    *   処理済みブロックを `temp/progress_{timestamp}.json` に保存
    *   エラー終了時に具体的な再開手順を表示
    *   例: `python -m src.cli.main run input.wav --resume temp/progress_20250120_103000.json`
*   **ログ記録:** `logs/processing.log` に処理状況を記録

### Step 3: タイムスタンプ再計算 (Local)

#### 3-1. `[WORD: ]` タグの分離とJSON化
LLM出力からタグを分離し、表示テキストとアライメント情報を分けて管理する。

1. **LLM出力のパース:**
   ```python
   # LLM出力例
   "設定を開いて[WORD: 開いて]\nくださいね[WORD: くださいね]"

   # パース結果（JSON）
   [
     {"text": "設定を開いて", "anchor_word": "開いて"},
     {"text": "くださいね", "anchor_word": "くださいね"}
   ]
   ```

2. **タグ削除と17文字再検証:**
   - タグを除去した`text`部分で17文字カウントを再実施
   - **17文字カウント仕様（明確化）:**
     - `unicodedata.east_asian_width` を使用
     - Fullwidth/Wide: 1文字
     - Halfwidth/Narrow/Neutral: 0.5文字
     - 合計を四捨五入して17文字以内に収める
   - 超過した行は自動再分割し、`logs/resplit.log` に記録

3. **最終データ構造:**
   ```python
   [
     {
       "text": "設定を開いて",       # 表示用テキスト（タグなし）
       "anchor_word": "開いて",    # アライメント用アンカー
       "validated": True           # 17文字検証済み
     },
     ...
   ]
   ```

#### 3-2. タイムスタンプアライメント処理
整形済みテキストとStep 1の単語タイムスタンプを突き合わせる。

*   **アプローチ（フェーズ1: シンプル実装）:**
    1.  各行の `anchor_word` をStep 1のタイムスタンプリストから検索
    2.  **完全一致:** 該当単語の `end` 時刻を字幕行の終了時刻とする
    3.  **Fuzzy Matching（不一致時）:**
        - RapidFuzz で類似度90%以上の単語を特定
        - 該当単語の `end` 時刻を採用
    4.  **仮タイムスタンプ（失敗時）:**
        - 前の行の終了時刻 + 0.3秒を開始時刻とする
        - 次の単語の `start` 時刻（利用可能な場合）を終了時刻とする
        - `logs/alignment_warnings.json` に警告を記録
    5.  次の行の開始時間は、前の行の終了時間 + 0.1秒

*   **将来の拡張（フェーズ2: 失敗率5%以上の場合のみ）:**
    - 先頭・中央アンカーも追加（末尾→先頭→中央の順で試行）
    - Fuzzy Matching閾値を段階的に調整（末尾90%、先頭85%、中央80%）

*   **注意点:**
    *   LLMがフィラーを削除したり単語を変更すると、完全一致は困難
    *   Fuzzy Matchingの閾値調整が重要（まず90%で開始し、実データで調整）

### Step 4: SRT出力
*   決定したタイムコードとテキストをSRT形式でファイルに書き出す。
*   **SRTフォーマット:**
    ```
    1
    00:00:10,500 --> 00:00:11,200
    設定を開いて
    2
    00:00:11,200 --> 00:00:12,000
    くださいね
    ```
## 4. UI/UX 要件
### フェーズ1: コマンドライン版（優先実装）
#### 基本実行（Typer CLI）
```bash
python -m src.cli.main run <音声ファイル> [オプション]
```

#### 主なオプション（実装に合わせて更新）
- `--models kotoba,mlx` : 使用するランナーをカンマ区切り指定。未指定なら **MLX large-v3のみ** 実行。
- `--llm {google|openai|anthropic}` : LLM整形プロバイダー。**未指定なら整形・SRT出力を行わず、文字起こしJSONのみ保存**（Plan推奨は google）。
- `--rewrite / --no-rewrite` : 語尾リライトを有効化（デフォルトNoneでプロバイダー既定に従う）。
- `--align-thresholds 90,85,80` : RapidFuzz類似度の閾値リスト。
- `--align-gap 0.1` : 行間の最小ギャップ秒。
- `--align-fallback-padding 0.3` : フォールバック時に前行へ足す秒数。
- `--language ja` / `--chunk-size 30` などは各ランナーへ伝播。
- `--resume temp/progress_xxx.json` : 途中から再開。
- `--simulate/--no-simulate` : ランナーのシミュレーション切替（デフォルトON）。

#### 出力パス
- 音声×モデル×実行時刻ごとに `temp/poc_samples/{run_id}.json` を保存。
- LLM整形を実行した場合のみ `output/{run_id}.srt` を自動命名で保存（`--output` オプションは存在しない）。

#### 実行例
```bash
# MLXのみで文字起こし（整形なし）
python -m src.cli.main run samples/sample_audio.m4a

# MLXとOpenAI Whisperを実行し、Geminiで整形＋SRT出力
python -m src.cli.main run samples/sample_audio.m4a \
  --models mlx,openai \
  --llm google \
  --align-thresholds 90,85,80 \
  --align-gap 0.1

# リライト有効＋Anthropic
python -m src.cli.main run samples/sample_audio.m4a --llm anthropic --rewrite
```

#### 進捗表示
*   コンソールに「音声解析中...」「AI思考中 (ブロック 5/20)...」「ファイル生成中」を表示
*   tqdmなどでプログレスバー表示

#### 出力
*   LLM整形を実行した場合のみ `output/{音声名}_{モデル}_{日時}.srt` を自動保存（--outputオプションなし）
*   文字起こしJSONは `temp/poc_samples/{run_id}.json` に保存

### フェーズ4: GUIアプリ（将来実装｜Plan準拠）
友人にとっての使いやすさを考慮し、設定項目は最小限にする。

*   **メイン画面:**
    *   ファイル選択エリア（ドラッグ＆ドロップ対応）
    *   **実行ボタン**
    *   **進捗バー:** 「音声解析中...」「AI思考中...」「ファイル生成中」などのステータス表示。
*   **オプション設定（トグルスイッチ等）:**
    *   [ ] **語尾調整・リライトを行う**（デフォルトOFF：原文維持＋フィラー削除のみ）
    *   [ ] **高精度モード**（large-v3モデル使用。OFFの場合はmediumモデルで高速化）
*   **出力:**
    *   元の音声ファイルと同じフォルダに `filename_subtitle.srt` を保存。
    *   完了時に通知を表示

## 5. プロンプト設計案（コアロジック）
OpenAI/Google/Anthropicの各LLMに送信する指示のプロトタイプです。
基本的なプロンプトは共通で、各プロバイダーのAPI仕様に合わせて送信します。

### 基本プロンプト（リライトなし）

```
【役割】
あなたはプロの動画編集者です。
【指示】
以下の音声認識テキストを、動画テロップ用に整形してください。
【制約条件】
1. **1行あたり全角17文字以内**に収めること。
2. 文脈を読み、**「意味のまとまり」や「息継ぎのタイミング」として自然な位置**で改行すること。
3. 「えー」「あー」「まあ」「その」などのフィラーは削除すること。
4. **原文の単語は極力変更しないこと**。表記ゆれのみ修正可（例：「出来る」→「できる」）。
5. 各行の最後に、その行の最後の単語を `[WORD: 単語]` の形式で必ず付記すること。
6. 出力は整形済みテキストのみ。説明文は不要。
【出力フォーマット例】
設定を開いて[WORD: 開いて]
くださいね[WORD: くださいね]
【入力テキスト】
{transcribed_text}
```

### リライトありプロンプト（オプション）

```
【役割】
あなたはプロの動画編集者です。
【指示】
以下の音声認識テキストを、動画テロップ用に整形してください。
【制約条件】
1. **1行あたり全角17文字以内**に収めること。
2. 文脈を読み、**「意味のまとまり」や「息継ぎのタイミング」として自然な位置**で改行すること。
3. 「えー」「あー」「まあ」「その」などのフィラーは削除すること。
4. 語尾を整え、読みやすく調整すること（例：「〜っていう感じで」→「〜です」）。
5. 各行の最後に、その行の最後の単語を `[WORD: 単語]` の形式で必ず付記すること。
   - リライトした場合、**元の音声で実際に発話された単語**をWORDタグに記載すること。
6. 出力は整形済みテキストのみ。説明文は不要。
【出力フォーマット例】
設定を開いてください[WORD: くださいね]
（元の音声: 「設定を開いてくださいね」を「設定を開いてください」にリライト）
【入力テキスト】
{transcribed_text}
```

### プロンプト設計のポイント
*   `[WORD: xxx]` タグは、タイムスタンプアライメントに必須
*   リライト時も、元の単語をWORDタグに記載することで、アライメント精度を維持
*   フィラー削除は両方のモードで実行（視聴体験向上のため）

---

## 6. 開発ロードマップ（Plan準拠）

### フェーズ1: コアロジック実装（完了間近）
**目標:** CLIで音声 → SRT を自動生成できるPoCを確立
- 3モデル（mlxデフォルト・kotoba・openai）実装と比較
- 進捗JSON・SRT出力・エラーハンドリングの統合

### フェーズ2: LLM整形＋タイムスタンプアライメント精度向上（進行中）
- `[WORD: ]` タグを前提にRapidFuzz閾値をチューニング（`--align-thresholds`, `--align-gap`）
- 実データで17文字制約と自然改行の精度評価、プロンプト改善
- LLMプロバイダー（推奨: Google）で整形を安定化

### フェーズ3: CLI UX / 再開機構強化
- `--resume` フローの長尺E2Eスモーク
- ログ整備と cleanup サブコマンド
- 実行メッセージ/ヘルプを初心者向けに簡潔化

### フェーズ4: GUIプロトタイプ
- Tkinter / Flet でドラッグ＆ドロップ + 進捗バー
- 最小限のオプション（モデル選択・リライト有無）に限定

### フェーズ5: パッケージ化（GUI後）
- PyInstaller / py2app 等で .app 化し、友人環境で配布確認

---

## 7. 技術的リスクと対策

| リスク | 影響度 | 対策 |
|--------|--------|------|
| タイムスタンプアライメント精度が低い | 高 | Fuzzy Matching閾値調整（まず90%で開始）、失敗率5%超なら複数アンカー導入 |
| LLMが17文字制約を守らない | 中 | プロンプト改善、タグ削除後の17文字再検証と自動再分割 |
| LLMが `[WORD: ]` タグを付け忘れる | 中 | プロンプトで明示的に指示、パース時に警告ログ出力 |
| 特定のLLMプロバイダーが障害 | 中 | 3社から選択可能にし、障害時は別プロバイダーへ切り替え |
| LLM APIコストが高騰 | 低 | 各社の低コストモデルを.envでデフォルト設定（gpt-4o-mini, gemini-1.5-flash等） |
| M3 MacでのGPU加速が不安定 | 中 | MLXが落ちる場合は openai-whisper をCPU/MPSで実行する |
| 長時間音声でメモリ不足 | 中 | ブロック分割処理（1200文字/30秒単位）、バッチサイズ調整 |
| API失敗時の処理中断 | 中 | 進捗を`temp/progress_*.json`に保存、--resumeオプションで再開可能に |
