# テロップ自動生成ツール　アプリ名:Flow Cut
 要件定義書
**Target Device:** Mac M3 (16GB Memory)
**User:** 友人（非エンジニア）

## 1. システム概要
音声ファイルをドラッグ＆ドロップするだけで、**「1行17文字以内」**かつ**「文脈として自然な区切り」**で構成されたSRT（字幕ファイル）を出力する自動化ツール。
従来ツールのような機械的な分割を排除し、プロの編集者が手動で切ったような「意味のまとまり」を再現することを最重要視する。

## 2. 技術スタック

### 音声認識（3モデルで比較検証）
プロトタイプで3つのモデルを実装し、速度・精度・word-levelタイムスタンプの品質を比較して最適なものを選定する。

*   **モデル1: kotoba-whisper-v2.0-mlx（日本語特化×MLX最適化）**
    *   リポジトリ: `kaiinui/kotoba-whisper-v2.0-mlx`
    *   日本語に特化したdistil-large-v3ベース
    *   Apple Silicon（M3）でMetal GPU加速
    *   word-levelタイムスタンプ対応
    *   ReazonSpeech 720万クリップで学習
    *   **期待値**: 最速＆日本語精度高

*   **モデル2: mlx-whisper large-v3（汎用×MLX最適化）**
    *   リポジトリ: `mlx-community/whisper-large-v3-mlx`
    *   OpenAI公式large-v3のMLX版
    *   Apple Silicon（M3）でMetal GPU加速
    *   word-levelタイムスタンプ対応
    *   多言語対応で汎用性高
    *   **期待値**: 最高精度（専門用語・複雑な内容）

*   **モデル3: OpenAI Whisper large-v3（公式実装）**
    *   リポジトリ: `openai/whisper`
    *   OpenAI公式実装
    *   MPS（Metal Performance Shaders）対応
    *   word-levelタイムスタンプ対応（別ライブラリ使用可能性あり）
    *   最も広く使われている標準実装
    *   **期待値**: 安定性・互換性重視のベースライン

### 文章整形・分割（LLM API）
以下のプロバイダーから選択可能。使用するモデルは`.env`ファイルで事前設定する。

*   **OpenAI** (gpt-4o, gpt-4o-mini など)
*   **Google** (gemini-1.5-pro, gemini-1.5-flash など)
*   **Anthropic** (claude-3-5-sonnet-20241022, claude-3-5-haiku-20241022 など)

**設計方針:**
*   各プロバイダーのAPIキーとモデル名を`.env`ファイルで管理
*   CLIオプション `--llm {openai|google|anthropic}` で実行時に切り替え
*   非エンジニアでもモデル更新可能（コード変更不要）
*   原文の単語を極力変更せず、アライメント精度を向上

**.env設定例:**
```env
# OpenAI
OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxx
OPENAI_MODEL=gpt-4o-mini

# Google Gemini
GOOGLE_API_KEY=AIzaSyxxxxxxxxxxxxxxxxx
GOOGLE_MODEL=gemini-1.5-flash

# Anthropic Claude
ANTHROPIC_API_KEY=sk-ant-xxxxxxxxxxxxx
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
```

### 言語・環境
*   **Python 3.10-3.12**（3.13は非対応）
*   **開発環境:** Python仮想環境（venv）
*   **Docker不使用の理由:**
    *   MLXはApple SiliconのMetal GPUに直接アクセスする必要がある
    *   DockerはMetal GPUサポートなし（CPU動作になり性能が大幅低下）
    *   ネイティブ環境が最も高速で開発効率も高い

### 依存関係管理
*   **フェーズ1（推奨）:** `pip + venv + requirements.txt`
    *   シンプルで初心者向け
    *   Python標準ツール（追加インストール不要）
    *   このプロジェクト規模では十分
*   **将来的な選択肢:** uv または Poetry
    *   プロジェクトが大規模化した際に検討
    *   バージョン競合の自動解決が必要になったら移行
*   **同一環境に3モデルを共存:**
    *   mlx-whisper（kotoba含む）
    *   openai-whisper（公式）
    *   faster-whisper（比較検証用）
    *   競合が発生した場合のみ環境分離を検討

### GUI（フェーズ2で実装）
*   **フェーズ1:** GUIなし（Pythonスクリプトのみ）
*   **フェーズ2:** 簡易GUI（Tkinter または Flet）
*   **将来的な選択肢:** Tauri + React（モダンなUI）

## 3. 処理フロー（内部ロジック）

### Step 1: 精密文字起こし (Local)
*   mlx-whisper（または faster-whisper）を使用し、音声からテキストデータを作成。
*   **必須:** `word_timestamps=True`を指定して、単語ごとの開始・終了時刻を取得。
    *   例: `{'word': '設定を', 'start': 10.5, 'end': 11.2}, ...`
*   **出力データ:**
    *   全文テキスト
    *   単語レベルのタイムスタンプリスト（JSON形式で保持）

### Step 2: 文脈理解と分割指示 (Cloud API)

#### ブロック分割戦略（長時間音声対応）
30分前後の動画にも対応するため、文字起こしテキストをブロック単位で処理する。

*   **ブロック長の上限:**
    *   **文字数**: 日本語1200文字（約600トークン）
    *   **音声時間**: 30秒
    *   いずれかに達した時点でブロック分割
*   **分割タイミング:** 単語のタイムスタンプを参照し、文章の切れ目で分割
*   **文脈維持:** 各ブロック末尾の最後の1-2文を次ブロック冒頭にも含める（重複ウィンドウ）
    *   整形後は行IDで重複除去し、文脈断絶を防止
*   **進捗表示:** 「ブロック 5/20 処理中...」のように進捗を表示

#### LLM API送信とプロンプト
*   ブロックごとに選択したLLM API（OpenAI/Google/Anthropic）へ送信
*   **プロンプトの役割:**
    1.  フィラー（えー、あー等）の削除。
    2.  **「1行17文字以内」**の制約を適用。
    3.  文法・意味構造解析による**「自然な改行位置」**の決定。
        *   *ルール:* 助詞の後ろ、読点、意味の切れ目を優先。
    4.  **重要:** 原文の単語は極力変更しない（表記ゆれのみ修正可）。
    5.  **出力形式:** 各行の最後に、その行の最後の単語を `[WORD: 単語]` の形式で付記。
    6.  （オプションON時のみ）語尾の微調整・リライト。

#### API出力例
```
設定を開いて[WORD: 開いて]
くださいね[WORD: くださいね]
```

#### エラーハンドリング
*   **リトライ戦略:** API失敗時は指数バックオフ（1s → 3s → 5s）で最大3回リトライ
*   **失敗時の動作:**
    *   処理済みブロックを `temp/progress_{timestamp}.json` に保存
    *   エラー終了時に具体的な再開手順を表示
    *   例: `python main.py input.wav --resume temp/progress_20250120_103000.json`
*   **ログ記録:** `logs/processing.log` に処理状況を記録

### Step 3: タイムスタンプ再計算 (Local)

#### 3-1. `[WORD: ]` タグの分離とJSON化
LLM出力からタグを分離し、表示テキストとアライメント情報を分けて管理する。

1. **LLM出力のパース:**
   ```python
   # LLM出力例
   "設定を開いて[WORD: 開いて]\nくださいね[WORD: くださいね]"

   # パース結果（JSON）
   [
     {"text": "設定を開いて", "anchor_word": "開いて"},
     {"text": "くださいね", "anchor_word": "くださいね"}
   ]
   ```

2. **タグ削除と17文字再検証:**
   - タグを除去した`text`部分で17文字カウントを再実施
   - **17文字カウント仕様（明確化）:**
     - `unicodedata.east_asian_width` を使用
     - Fullwidth/Wide: 1文字
     - Halfwidth/Narrow/Neutral: 0.5文字
     - 合計を四捨五入して17文字以内に収める
   - 超過した行は自動再分割し、`logs/resplit.log` に記録

3. **最終データ構造:**
   ```python
   [
     {
       "text": "設定を開いて",       # 表示用テキスト（タグなし）
       "anchor_word": "開いて",    # アライメント用アンカー
       "validated": True           # 17文字検証済み
     },
     ...
   ]
   ```

#### 3-2. タイムスタンプアライメント処理
整形済みテキストとStep 1の単語タイムスタンプを突き合わせる。

*   **アプローチ（フェーズ1: シンプル実装）:**
    1.  各行の `anchor_word` をStep 1のタイムスタンプリストから検索
    2.  **完全一致:** 該当単語の `end` 時刻を字幕行の終了時刻とする
    3.  **Fuzzy Matching（不一致時）:**
        - RapidFuzz で類似度90%以上の単語を特定
        - 該当単語の `end` 時刻を採用
    4.  **仮タイムスタンプ（失敗時）:**
        - 前の行の終了時刻 + 0.3秒を開始時刻とする
        - 次の単語の `start` 時刻（利用可能な場合）を終了時刻とする
        - `logs/alignment_warnings.json` に警告を記録
    5.  次の行の開始時間は、前の行の終了時間 + 0.1秒

*   **将来の拡張（フェーズ2: 失敗率5%以上の場合のみ）:**
    - 先頭・中央アンカーも追加（末尾→先頭→中央の順で試行）
    - Fuzzy Matching閾値を段階的に調整（末尾90%、先頭85%、中央80%）

*   **注意点:**
    *   LLMがフィラーを削除したり単語を変更すると、完全一致は困難
    *   Fuzzy Matchingの閾値調整が重要（まず90%で開始し、実データで調整）

### Step 4: SRT出力
*   決定したタイムコードとテキストをSRT形式でファイルに書き出す。
*   **SRTフォーマット:**
    ```
    1
    00:00:10,500 --> 00:00:11,200
    設定を開いて

    2
    00:00:11,200 --> 00:00:12,000
    くださいね
    ```

## 4. UI/UX 要件

### フェーズ1: コマンドライン版（優先実装）

#### 基本実行
```bash
python main.py input.wav [オプション]
```

#### オプション一覧

##### Whisperモデル選択
```bash
--whisper-model {kotoba|mlx|openai}
```
- `kotoba`: 日本語特化（推奨） - `kaiinui/kotoba-whisper-v2.0-mlx`
- `mlx`: 汎用高精度 - `mlx-community/whisper-large-v3-mlx`
- `openai`: 標準実装 - `openai/whisper large-v3`
- **デフォルト:** `kotoba`

##### LLMプロバイダー選択
```bash
--llm {openai|google|anthropic}
```
- 使用するモデルは `.env` ファイルで事前設定
- **デフォルト:** `openai`
- **例:** `python main.py input.wav --llm google`

##### その他のオプション
- `--rewrite`: 語尾調整・リライトを行う（デフォルトOFF）
- `--output`: 出力ファイルパス（デフォルト: `input_subtitle.srt`）
- `--resume`: 中断した処理を再開（`--resume temp/progress_xxxx.json`）

#### 実行例
```bash
# 基本実行（デフォルト設定）
python main.py input.wav

# Google Geminiを使用
python main.py input.wav --llm google

# リライトありで実行
python main.py input.wav --rewrite

# Whisperモデル変更
python main.py input.wav --whisper-model mlx

# すべて指定
python main.py input.wav --llm anthropic --whisper-model openai --rewrite --output result.srt
```

#### 進捗表示
*   コンソールに「音声解析中...」「AI思考中 (ブロック 5/20)...」「ファイル生成中」を表示
*   tqdmなどでプログレスバー表示

#### 出力
*   元の音声ファイルと同じフォルダに `filename_subtitle.srt` を保存

### フェーズ2: GUIアプリ（将来実装）
友人にとっての使いやすさを考慮し、設定項目は最小限にする。

*   **メイン画面:**
    *   ファイル選択エリア（ドラッグ＆ドロップ対応）
    *   **実行ボタン**
    *   **進捗バー:** 「音声解析中...」「AI思考中...」「ファイル生成中」などのステータス表示。
*   **オプション設定（トグルスイッチ等）:**
    *   [ ] **語尾調整・リライトを行う**（デフォルトOFF：原文維持＋フィラー削除のみ）
    *   [ ] **高精度モード**（large-v3モデル使用。OFFの場合はmediumモデルで高速化）
*   **出力:**
    *   元の音声ファイルと同じフォルダに `filename_subtitle.srt` を保存。
    *   完了時に通知を表示

## 5. プロンプト設計案（コアロジック）
OpenAI/Google/Anthropicの各LLMに送信する指示のプロトタイプです。
基本的なプロンプトは共通で、各プロバイダーのAPI仕様に合わせて送信します。

### 基本プロンプト（リライトなし）

```
【役割】
あなたはプロの動画編集者です。

【指示】
以下の音声認識テキストを、動画テロップ用に整形してください。

【制約条件】
1. **1行あたり全角17文字以内**に収めること。
2. 文脈を読み、**「意味のまとまり」や「息継ぎのタイミング」として自然な位置**で改行すること。
3. 「えー」「あー」「まあ」「その」などのフィラーは削除すること。
4. **原文の単語は極力変更しないこと**。表記ゆれのみ修正可（例：「出来る」→「できる」）。
5. 各行の最後に、その行の最後の単語を `[WORD: 単語]` の形式で必ず付記すること。
6. 出力は整形済みテキストのみ。説明文は不要。

【出力フォーマット例】
設定を開いて[WORD: 開いて]
くださいね[WORD: くださいね]

【入力テキスト】
{transcribed_text}
```

### リライトありプロンプト（オプション）

```
【役割】
あなたはプロの動画編集者です。

【指示】
以下の音声認識テキストを、動画テロップ用に整形してください。

【制約条件】
1. **1行あたり全角17文字以内**に収めること。
2. 文脈を読み、**「意味のまとまり」や「息継ぎのタイミング」として自然な位置**で改行すること。
3. 「えー」「あー」「まあ」「その」などのフィラーは削除すること。
4. 語尾を整え、読みやすく調整すること（例：「〜っていう感じで」→「〜です」）。
5. 各行の最後に、その行の最後の単語を `[WORD: 単語]` の形式で必ず付記すること。
   - リライトした場合、**元の音声で実際に発話された単語**をWORDタグに記載すること。
6. 出力は整形済みテキストのみ。説明文は不要。

【出力フォーマット例】
設定を開いてください[WORD: くださいね]
（元の音声: 「設定を開いてくださいね」を「設定を開いてください」にリライト）

【入力テキスト】
{transcribed_text}
```

### プロンプト設計のポイント
*   `[WORD: xxx]` タグは、タイムスタンプアライメントに必須
*   リライト時も、元の単語をWORDタグに記載することで、アライメント精度を維持
*   フィラー削除は両方のモードで実行（視聴体験向上のため）

---

## 6. 開発ロードマップ

### フェーズ1: コアロジック実装（最優先）
**目標:** GUIなしのPythonスクリプトで、音声ファイル → SRTファイルの自動生成を実現

#### ステップ1-1: 音声認識の検証（3モデル比較）
*   **3モデルの実装:**
    1.  kotoba-whisper-v2.0-mlx（日本語特化×MLX）
    2.  mlx-whisper large-v3（汎用×MLX）
    3.  OpenAI Whisper large-v3（公式実装）
*   **評価項目:**
    *   日本語音声での文字起こし精度
    *   処理速度（リアルタイム倍率）
    *   word-levelタイムスタンプの品質（単語境界の正確性）
    *   メモリ使用量
*   **テストデータ:** 友人の実際の動画音声（5-10分程度）
*   **採用モデルの決定:** 総合評価で最適なモデルを選択

#### ステップ1-2: プロンプトとアライメントの検証
*   音声認識結果をOpenAI APIで整形（基本プロンプトを使用）
*   `[WORD: xxx]` タグを使ったタイムスタンプアライメントの実装
*   Fuzzy Matching（RapidFuzz）の精度テスト
*   **「17文字・自然な区切り」の実現度を確認、プロンプト調整**

#### ステップ1-3: 統合とSRT出力
*   Step 1〜4を統合した完全なパイプラインを実装
*   エラーハンドリング（音声ファイル形式、API失敗時など）
*   進捗表示（tqdm等）
*   実際の音声ファイルで動作確認

#### 成果物
*   `main.py`: コマンドライン実行可能なスクリプト
*   `README.md`: 使い方、セットアップ手順
*   `.env.example`: 環境変数のテンプレート（APIキー設定例）
*   `requirements.txt`: 必要なPythonパッケージ一覧

---

### フェーズ2: GUI実装（ロジック完成後）
**目標:** 友人が使いやすいGUIアプリ化

*   Tkinter または Flet でシンプルなGUIを実装
*   ドラッグ＆ドロップ対応
*   進捗バー表示
*   オプション設定（リライト有無、モデル選択）

---

### フェーズ3: パッケージ化（GUI完成後）
**目標:** Mac M3で.appファイルとして配布可能にする

*   PyInstaller または py2app でバイナリ化
*   友人のMacで動作確認
*   必要に応じてインストールガイド作成

---

## 7. 技術的リスクと対策

| リスク | 影響度 | 対策 |
|--------|--------|------|
| タイムスタンプアライメント精度が低い | 高 | Fuzzy Matching閾値調整（まず90%で開始）、失敗率5%超なら複数アンカー導入 |
| LLMが17文字制約を守らない | 中 | プロンプト改善、タグ削除後の17文字再検証と自動再分割 |
| LLMが `[WORD: ]` タグを付け忘れる | 中 | プロンプトで明示的に指示、パース時に警告ログ出力 |
| 特定のLLMプロバイダーが障害 | 中 | 3社から選択可能にし、障害時は別プロバイダーへ切り替え |
| LLM APIコストが高騰 | 低 | 各社の低コストモデルを.envでデフォルト設定（gpt-4o-mini, gemini-1.5-flash等） |
| M3 MacでのGPU加速が不安定 | 中 | faster-whisper（CPU版）にフォールバック |
| 長時間音声でメモリ不足 | 中 | ブロック分割処理（1200文字/30秒単位）、バッチサイズ調整 |
| API失敗時の処理中断 | 中 | 進捗を`temp/progress_*.json`に保存、--resumeオプションで再開可能に |

---

## 次のアクション

**まずは「フェーズ1: ステップ1-1」から開始します。**

音声認識ライブラリの選定と検証を行い、最適な技術スタックを確定させましょう。