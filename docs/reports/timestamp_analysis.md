# SRTタイムスタンプ破損問題の分析

## 問題の概要
ユーザーより、SRTファイルのインデックス13付近からタイムスタンプがめちゃくちゃになるという報告がありました。
- **SRT Line 12**: `00:00:17,460 --> 00:00:18,460` (テキスト: "じゃないですか")
- **SRT Line 13**: `00:00:12,480 --> 00:00:12,680` (テキスト: "何もないから") **<-- 時間が巻き戻っている**

## データ分析

### 入力JSONのタイムスタンプ順序異常
`grep` コマンドによる調査の結果、入力されたJSONファイル（Whisperの出力）自体において、単語の出現順と時間の順序が逆転していることが確認されました。

1.  **"じゃないですか"** (SRT Line 12):
    - JSON内の行番号: 215行目付近
    - タイムスタンプ: `start: 17.46`, `end: 18.46`

2.  **"何もないから"** (SRT Line 13):
    - JSON内の行番号: 225行目付近（"じゃないですか" の**後ろ**に出現）
    - タイムスタンプ: `start: 12.48`, `end: 12.68`

### 結論（訂正）
ユーザー様のご指摘通り、入力JSONのタイムスタンプは正しい順序で並んでいました。
- "何もないから" は正しく 19秒付近 に存在します。
- しかし、12秒付近に "ぐらいから" というフレーズがあり、ここにも "から" が含まれています。

## LLMの思考プロセスとエラーの再現
ユーザー様のご要望に基づき、LLMがどのようにしてこのエラー（インデックス指定ミス）を起こしたか、その内部処理をシミュレーションして解説します。

### 1. LLMへの入力（プロンプト）
LLMには、以下のような「インデックス付き単語リスト」が渡されます。（※行番号はJSON内の位置を元にした概算です）

```text
...
35: 1 (11.46s)
36: 月
37: ぐらい
38: から (12.46s - 12.68s)  <-- ★罠（Trap）
...
65: じゃない
66: ですか (18.16s - 18.38s)
67: 何
68: も
69: ない
70: から (19.30s - 19.48s)  <-- ★正解（Target）
...
```

### 2. LLMのタスク
LLMは、このリストを見て「読みやすい字幕の行」を作り、その「開始・終了インデックス」を指定するように指示されています。
LLMは以下のような行を作ろうとしました：
1.  「...じゃないですか」
2.  「何もないから」

### 3. エラー発生の瞬間（ハルシネーション）
LLMは「何もないから」という行を作る際、**「から」で終わる場所**を探します。
ここで、本来選ぶべき **インデックス70（19秒の「から」）** ではなく、少し前にあった **インデックス38（12秒の「から」）** に目が止まってしまいました。

LLMの内部思考（推測）:
> 「次は『何もないから』だな。よし、『から』で終わる場所を探そう。
> お、インデックス38に『から』があるぞ。ここまでの文脈的にも近いし、ここを終了点にしよう。」

その結果、LLMは以下のようなJSONを出力してしまいました：

```json
{
  "lines": [
    { "text": "...じゃないですか", "from": 60, "to": 66 },
    { "text": "何もないから",     "from": 67, "to": 38 }  // ← ここでミス！
  ]
}
```
※実際には `from` が `to` より大きい（67 > 38）とプログラムで弾かれるため、おそらく `from` も巻き込んで **`"from": 35, "to": 38` ("1月ぐらいから" の範囲)** を指定しつつ、テキストだけ **"何もないから"** と書き換えて出力した可能性が高いです。

### 4. 結果としてのSRT
プログラムは、LLMから渡されたインデックス（35〜38）を信じてタイムスタンプを取得し、テキストはLLMが書いた「何もないから」を使います。

- **テキスト**: "何もないから" （LLMの出力テキスト）
- **開始時間**: 11.46秒 （インデックス35の時間）
- **終了時間**: 12.68秒 （インデックス38の時間）

これにより、**「テキストは正しいのに、時間だけが12秒（過去）に飛ぶ」** という現象が完成します。

## Q&A: なぜ順番通りに処理しなかったのか？

**Q: LLMはインデックス1から順番に処理しているはずなのに、なぜ既に通過したはずの「38」に戻ってしまったのですか？**

**A: LLMは「プログラム」ではなく「確率的なテキスト生成器」であり、厳密な状態管理が苦手だからです。**

人間やプログラムなら「現在のカーソル位置は66。次は67から探す」という変数を管理できますが、LLM（Transformerモデル）にはそのような「隠し変数」や「カーソル」はありません。
LLMは常に **「入力された全文（プロンプト）」** と **「これまでに出力した文字」** の全体を見て、確率的に次の文字を決めています。

今回のケースでは、以下のような **「Attention Slip（注意の滑り）」** が起きたと考えられます：

1.  **強い類似性**: 「何もない**から**」と「1月ぐらい**から**」は、どちらも「から」で終わる文節であり、構造が似ています。
2.  **局所的な注目**: LLMが「『から』のインデックスは何番だっけ？」と入力テキストを見渡した際、本来見るべき「70番のから」よりも、視界の端（コンテキストウィンドウ内の少し前）にあった **「38番のから」** の方が、何らかの理由（位置的な近さや、他の単語との関連性など）で強く注意（Attention）を引いてしまいました。
3.  **制約の無視**: プロンプトで「順番に変えるな」と指示していても、LLMにとってはそれは「確率的なバイアス」に過ぎません。「38番」という数字を出力する確率が、「70番」を出力する確率を上回ってしまった瞬間、論理的な順序整合性よりも、その場の確率が優先されてしまいます。

このように、**「論理的にはありえない（既に終わったはずの場所に戻る）」** という挙動も、確率モデルであるLLMでは起こり得ます。だからこそ、今回実装したような **「プログラム側での厳密なチェック（クランプ処理）」** が不可欠となります。

### 対策の有効性
今回実装した **「タイムスタンプのクランプ（補正）」** は、このようなLLMのインデックス指定ミスに対しても有効です。
LLMが誤って過去のインデックス（12秒）を指定しても、クランプ処理によって「前の行の終了時刻（18秒）」まで強制的に引き上げられるため、SRTの時間が巻き戻ることを防げます。
