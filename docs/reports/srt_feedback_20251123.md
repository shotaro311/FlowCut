# SRT改善要望レポート

**対象ファイル**: `output/sample_audio_mlx_20251123T143709.srt`
**作成日**: 2025-11-23

## 概要
今回の出力結果に対する改善要望まとめ。
**共通課題**: テロップ行の始まりは自然な区切りで始めるべきで、自然な文章や文脈のまとまりを意識する必要がある。

---

## 改善要望リスト

### 1. 助詞・接続詞の不自然な分割

| ID | 該当行 | 現状のテキスト | 理想のテキスト / 要望 | 理由 |
| :--- | :--- | :--- | :--- | :--- |
| 1 | 9-10 | 「やって」→「たわけじゃない」 | 「やってた」をひとまとまりに | 「やってた」が不自然に分割されて読みにくい |
| 2 | 15-16 | 「考えた」→「ことがあったから」 | 「考えたことが」をひとまとまりに | 助詞「が」まで含めた方が自然 |
| 3 | 18-19 | 「じゃないかな」→「と思って」 | 「じゃないかなと思って」をひとまとまりに | 引用の「と」で分断されるのが不自然 |
| 4 | 23-24 | 「時」→「にいや」 | 「就職しないのかって時に」をひとまとまりに | 「時に」で分割されているのが不自然 |
| 5 | 65-66 | 「あった」→「ので」 | 「あったので」をひとまとまりに | 接続助詞「ので」で分断されるのが不自然 |
| 6 | 84-85 | 「目指す」→「ものの」 | 「目指すものの」をひとまとまりに | 逆接の接続助詞「ものの」で分断されるのが不自然 |
| 7 | 95-96 | 「帰ってきたとした」→「ら」 | 「帰ってきたとしたら」をひとまとまりに | 仮定表現「たら」で分断されるのが不自然 |
| 8 | 105-106 | 「八ヶ谷先生だ」→「ったと思います」 | 「八ヶ谷先生だったと」をひとまとまりに | 過去形「だったと」で分断されるのが不自然 |

### 2. 極端に短い行（1文字~数文字）

| ID | 該当行 | 現状のテキスト | 問題点 | 理想の対応 |
| :--- | :--- | :--- | :--- | :--- |
| 9 | 20-21 | 「なった」→「時」（1文字のみ） | 「時」だけで1行使っているのが明らかにおかしい | 前行と統合: 「なった時」 |

---

## 問題の傾向分析

### 主な問題パターン
1. **助詞での分割** (`が`, `に`, `で`, etc.)
   - 例: 「考えたこと**が**」「時**に**」「あった**ので**」
   
2. **接続表現での分割** (`〜と思って`, `〜ものの`, `〜たら`, etc.)
   - 例: 「じゃないかな**と思って**」「目指す**ものの**」「帰ってきた**としたら**」

3. **活用語尾での分割** (`〜た`, `〜だった`, etc.)
   - 例: 「やっ**てた**」「八ヶ谷先生だっ**たと**」

4. **極端に短い行の発生**
   - 1～2文字のみの行が発生（読み手の視線誘導が困難）

### 根本原因の推測
LLMが「17文字制約」を優先しすぎて、以下のような判断をしている可能性：
- 文法的なまとまり（フレーズ境界）よりも文字数を優先
- 日本語の自然な切れ目（ポーズ位置）を軽視
- 助詞や接続詞を「次の行の始まり」として許容してしまっている

---

## 全体的なフィードバック

### ✅ 良い点
- 全体的な文字数制約は守られている（17文字以内）
- タイムスタンプの精度が向上している（クランプ処理による）

### ❌ 改善が必要な点
- **助詞・接続詞での不自然な分割が頻発**
  - 特に「〜と思って」「〜ものの」「〜たら」「〜ので」「〜時に」など
- **読み手の視線誘導を妨げる極端に短い行**
  - 1～2文字の行は避けるべき

---

## 次のアクション

### Pass 2プロンプト修正（詳細版）

以下の修正を `src/llm/two_pass.py` の `_build_pass2_prompt` メソッドに適用します。

---

#### 📋 現在のプロンプト構造（抜粋）

```python
def _build_pass2_prompt(self, words: Sequence[WordTimestamp], *, max_chars: float) -> str:
    indexed = _build_indexed_words(words)
    return (
        "# Role\n"
        "あなたは熟練の動画テロップ編集者です。\n"
        "提供されたテキストを、視聴者が最も読みやすいリズムで読めるように、以下の【思考ワークフロー】に従って処理し、行のインデックス範囲を JSON で返してください。\n\n"
        "# Constraints (制約)\n"
        f"- 1行の最大文字数：全角{int(max_chars)}文字\n"
        "- 出力形式：JSON の lines 配列のみ（例を参照）\n"
        "- 禁止事項：行末の句読点（、。）は必ず削除すること。文中の句読点は残してもよい。\n"
        "- 単語の順序を変えない。結合もしない。\n\n"
        
        "# Thinking Workflow (思考ワークフロー)\n"
        "## Step 1: チャンク分解 (Chunking)\n"
        "入力されたテキストを、文節や意味の最小単位（チャンク）に分解する。句読点（、。）や接続助詞（〜て、〜が、〜ので、〜から）を強い区切りとして扱う。\n\n"
        "## Step 2: 行の構築と決定 (Line Building)\n"
        "チャンクを前から順に追加し、以下の判定を行う。\n"
        f"1. 文字数オーバー: 現バッファ＋次チャンクが{int(max_chars)}文字を超えるなら改行。\n"
        f"2. 文脈区切り: {int(max_chars)}文字以内でも、読点・強い切れ目（〜ます、〜です、〜だ等）・接続助詞で終わるなら改行。\n\n"
        "## Step 3: クリーニング (Cleaning)\n"
        "行末の句読点（、。）を削除。文中の句読点は残してよい。\n\n"
        
        # ... (Input/Output continue)
    )
```

---

#### ✨ 修正版プロンプト（追加箇所をハイライト）

```python
def _build_pass2_prompt(self, words: Sequence[WordTimestamp], *, max_chars: float) -> str:
    indexed = _build_indexed_words(words)
    return (
        "# Role\n"
        "あなたは熟練の動画テロップ編集者です。\n"
        "提供されたテキストを、視聴者が最も読みやすいリズムで読めるように、以下の【思考ワークフロー】に従って処理し、行のインデックス範囲を JSON で返してください。\n\n"
        
        "# Constraints (制約)\n"
        f"- 1行の最大文字数：全角{int(max_chars)}文字\n"
        "- 出力形式：JSON の lines 配列のみ（例を参照）\n"
        "- 単語の順序を変えない。結合もしない。\n\n"
        
        # ✅ 追加: 自然な分割ルール
        "# 自然な分割ルール（最優先）\n"
        "**以下のルールは文字数制約よりも優先度が高い：**\n"
        "1. **助詞で分割しない**: が、を、に、で、の、は、も、から、まで 等で行を終わらせない\n"
        "2. **接続表現で分割しない**: 〜と思って、〜ものの、〜たら、〜ので、〜けど、〜けれど 等で文を切らない\n"
        "3. **最小行長の確保**: 1行は最低でも3文字以上必要（1〜2文字のみの行は絶対に禁止）\n"
        "4. **活用語尾の保持**: 〜てた、〜だった、〜たと 等の活用形は分割せずひとまとまりに\n\n"
        
        # ✅ 追加: 良い例・悪い例
        "# 分割の良い例・悪い例\n"
        "❌ 悪い例:\n"
        "  - 「考えた」→「ことがあったから」 （助詞「が」で分断）\n"
        "  - 「なった」→「時」 （1文字のみの行）\n"
        "  - 「目指す」→「ものの」 （接続助詞で分断）\n\n"
        "✅ 良い例:\n"
        "  - 「考えたことが」→「あったから」 （助詞を含めてひとまとまり）\n"
        "  - 「なった時に」→「いやしないと」 （最小3文字以上）\n"
        "  - 「目指すものの」→「ダメな場合も」 （接続表現を保持）\n\n"
        
        # 修正: 禁止事項を統合
        "# 禁止事項\n"
        "- 行末の句読点（、。）は必ず削除すること。文中の句読点は残してもよい。\n"
        "- 助詞・接続詞・活用語尾での不自然な分割（上記ルール参照）\n"
        "- 1〜2文字のみの極端に短い行\n\n"
        
        "# Thinking Workflow (思考ワークフロー)\n"
        "## Step 1: チャンク分解 (Chunking)\n"
        "入力されたテキストを、文節や意味の最小単位（チャンク）に分解する。句読点（、。）や接続助詞（〜て、〜が、〜ので、〜から）を強い区切りとして扱う。\n\n"
        
        # ✅ 修正: 優先順位を明示
        "## Step 2: 行の構築と決定 (Line Building)\n"
        "チャンクを前から順に追加し、以下の **優先順位** で判定を行う。\n"
        "**【最優先】自然な意味のまとまり（フレーズ境界）** を保持する\n"
        f"1. 助詞・接続表現での分割を避ける（上記ルール参照）\n"
        f"2. 文字数オーバー: 現バッファ＋次チャンクが{int(max_chars)}文字を超える場合のみ改行\n"
        f"3. 文脈区切り: {int(max_chars)}文字以内でも、読点・強い切れ目（〜ます、〜です、〜だ等）で終わるなら改行を検討\n"
        "4. 最小行長チェック: 分割後の行が3文字未満にならないか確認\n\n"
        
        "## Step 3: クリーニング (Cleaning)\n"
        "行末の句読点（、。）を削除。文中の句読点は残してよい。\n\n"
        
        "# Input\n"
        f"単語リスト（index:word）:\n{indexed}\n\n"
        
        "# Output\n"
        '以下のJSONだけを返してください:\n{"lines":[{"from":0,"to":10,"text":"私は大学の12月ぐらい"},{"from":11,"to":25,"text":"政治家になろうと決めていて"}]}'
    )
```

---

#### 📝 修正内容まとめ

| 修正箇所 | 修正内容 | 目的 |
| :--- | :--- | :--- |
| **新セクション追加** | 「# 自然な分割ルール（最優先）」 | 助詞・接続詞での分割禁止を明示 |
| **例示追加** | 「# 分割の良い例・悪い例」 | 具体例でLLMの理解を促進 |
| **Step 2修正** | 優先順位を番号付きリストで明記 | 「自然さ > 文字数」の優先度を強調 |
| **禁止事項統合** | 句読点削除ルールと新ルールを統合 | 一箇所で確認できるように整理 |
| **最小行長の明示** | 「3文字以上」を複数箇所で強調 | 極端に短い行の防止 |

---

### パラメータ調整（オプション）

現時点では不要と判断。プロンプト修正の効果を確認後、以下を検討：

- [ ] `max_chars` を17→18に緩和（柔軟性を持たせる）
- [ ] `temperature` を調整（より保守的な分割を促す）

### 後処理ロジック追加（将来的な拡張案）

プロンプト修正で改善が不十分な場合の保険として：

- [ ] **極端に短い行の検出・統合**: 生成後に1〜2文字の行を検出し、前行と自動統合
- [ ] **助詞終わりの行の検出**: 助詞で終わる行を検出し、警告ログを出力
