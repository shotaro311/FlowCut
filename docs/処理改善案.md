# テロップ処理 改善メモ（ドラフト）

このドキュメントは、FlowCut のテロップ生成ワークフローについて、
これまで議論した内容と改善アイデアを簡単に整理したメモです。

## 1. 現状ワークフローの前提

- **元データ**: Whisper / MLX によるワード単位の文字起こし結果（`WordTimestamp`）
  - 単語ごとの `word`, `start`, `end`, `confidence` を保持
- **大前提**: 
  - **ワードレベルのタイムコードを崩さない**
  - **単語の順序を変えない**
 - **テロップ制約**:詳しくは[two_pass.py](../src/llm/two_pass.py)のpassを参照
  - 1 行あたり **全角 5〜17 文字** に収める
  - 極端に短い行（1〜4 文字）を作らない
  - 助詞・接続助詞・終助詞などで不自然に行を切らない
  - 引用表現（〜って言う／〜って思う など）は途中で切らない

## 2. TwoPassFormatter のパス構成（実装ベース）

実装上は 4 パス構成になっている。

1. **Pass1: 誤認識修正（replace/delete）**
   - 入力: 元テキスト + `words`
   - 出力: `operations` 配列（replace/delete のみ）
   - 目的: Whisper 誤認識の修正（挿入は禁止、語順も変えない）

2. **Pass2: 行分割（line splitting）**
   - 入力: Pass1 適用後の `updated_words`
   - 出力: `lines` 配列（`from`/`to`/`text`）
   - 目的: 5〜17 文字制約と日本語として自然な区切りを両立する行分割

3. **Pass3: 検証 & 最小修正**
   - `validators.detect_issues` で問題行を検出
   - LLM に現在の行分割と issues を渡し、
     - 短すぎる行の統合
     - 引用表現の分割修正
     - 語の途中で切れている箇所の連結
     などを **必要最小限の範囲で修正** させる。

4. **Pass4: 長さ違反行の再分割**
   - `len(line.text) > 17 or < 5` の行に対してのみ LLM を再実行
   - 現状は **対象行ごとに 1 リクエストずつ** 呼び出しているため、
     対象行が多いと Pass4 の総時間が大きくなりがち。

## 3. 現状テストで見えている主な課題

1. **行の区切りの不自然さ**
   - 助詞・接続助詞・終助詞の直前／直後での分割が完全には防ぎ切れていない。
   - 5〜17 文字制約から僅かに外れる行や、意味的につながりが不自然な行が一部残る。

2. **固有名詞（特に人名など）の漢字誤り**
   - 例: 政治家名・組織名・学校名などで、
     読みは合っているが漢字が誤っているケースが残る。
   - Whisper の誤認識に加え、LLM 側の変換でも誤った漢字を選ぶ場合がある。

3. **Pass3 / Pass4 の処理時間**
   - Pass3 は常時実行の設計になっており、問題が少ない場合でも必ず LLM を 1 回呼ぶ。
   - Pass4 は「違反行ごとに 1 リクエスト」のため、対象行が増えるとリクエスト数が行数オーダーで増える。
   - ただし、本プロジェクトでは **精度（A: 行区切り / B: 固有名詞）を最優先** とし、
     時間・コストは二次的とする方針で議論を進めている。

## 4. 精度向上のための議論ポイント

### 4-1. Pass1（誤認識・固有名詞修正）の強化

- 現状 Pass1 は「最小限の replace/delete で音声に忠実にする」設計。
- 今後の方向性案:
  - **固有名詞修正に重心を置いたプロンプト** に調整する。
    - 政治家名・政党名・官庁名・大学名などは、可能な範囲で実在の正式表記に揃える。
    - 自信がない場合は変更しない（誤った漢字への置き換えを避ける）。
    - 一般に知られていない個人名（生徒名など）は、むしろ安易に変えない方向。
  - 操作制約（insert 禁止 / 語順固定）は維持し、
    Whisper のタイムコードとアラインメントを壊さない。

### 4-2. Pass2〜4 のプロンプト調整

- **Pass2**: 行分割のルールと例示を整理し、
  - 優先順位（絶対ルール / 分割禁止位置 / 分割推奨位置）を明確にする。
  - 良い例・悪い例をパターン別に増やし、日本語の区切り感覚を強化する。

- **Pass3**: 
  - 役割は「Pass2 の結果を最小限修正するチェッカー」として維持。
  - issues が 0 件のときにスキップするかどうかは、精度とコストのトレードオフとして別途検討。

- **Pass4**:
  - 現状は行ごとに LLM 呼び出し → 対象行が多いと時間が伸びる。
  - 改善案として「複数行をまとめて 1 回のプロンプトで処理する **バッチ化**」を検討中。
    - 各行に `line_id` を振り、
      `{ "items": [ { "line_id": 0, "lines": [...] }, ... ] }` の形式で返させる。
    - Python 側で `line_id` ごとに分解し、元の `LineRange` に対応付けるパーサを実装するイメージ。

## 5. 形態素解析導入のアイデア

現状の FlowCut では Sudachi などの形態素解析器は **未使用**。
今後、以下のような使い方を検討できる余地がある。

- **目的**:
  - 行分割時に「助詞・接続助詞・終助詞・引用表現」などを機械的に検出し、
    不自然な区切りをルールベースで抑制する。
  - 文節（チャンク）境界を事前にマークし、LLM の負担を減らしつつ精度を上げる。

- 想定するパイプライン案:
  1. Pass1 などで固有名詞をある程度正しい表記にそろえる。
  2. その結果テキストを **Sudachi などで形態素解析** し、
     - 品詞（名詞 / 助詞 / 動詞 / 終助詞 など）
     - 文節境界
     を取得。
  3. 取得した情報をもとに、行分割の候補境界や NG 境界をルール化。
  4. LLM（Pass2/3）には、このルール情報も添えて「ここは切るな／ここは切ってよい」とガイドする。

これにより、LLM のプロンプトを過剰に複雑にしなくても、
日本語として不自然な区切りを減らせる可能性がある。

## 6. 外部ライブラリ・API についてのメモ

- **Vercel AI SDK**
  - TypeScript/Next.js 向けの SDK であり、Python バックエンド中心の FlowCut には直接は関係しない。
  - Web フロントエンドを Next.js で作る場合などに検討余地があるが、
    現時点のテロップ処理精度には直結しないため優先度は低い。

- **Instructor（構造化出力ライブラリ）**
  - LLM から Pydantic モデルとして JSON を安全に受け取るための Python ライブラリ。
  - 効果があるのは主に「JSON が崩れてパースに失敗する」ケースの削減であり、
    行分割そのものの賢さには直接は効かない。
  - 現状は、Pass2〜4 のプロンプトとロジック改善の方がインパクトが大きいと判断し、
    Instructor 導入は「安定性向上のオプション」として後回しにしている。

## 7. 今後詰めたいポイント（メモ）

- **A. 行の区切り精度の最大化**
  - Pass2/3/4 のプロンプトとルールの再設計。
  - 形態素解析の導入タイミングと粒度（文節レベルか、品詞タグだけか）。

- **B. 固有名詞の漢字誤りの削減**
  - Pass1 プロンプトを「固有名詞修正寄り」に振るか、専用パスを追加するか。
  - LLM に「自信がない場合は変更しない」ポリシーを徹底させる。

- **C. 精度優先 vs. 処理時間のバランス**
  - 現時点では精度最優先だが、Pass4 バッチ化など「精度を落とさずに速くする」施策は並行して検討する。

（以上、2025-11-27 時点の議論ベースのドラフト。今後の検討に応じて追記・修正していく想定。）

