# プランニングドキュメント：Flow Cut コアCLI実装

**作成日**: 2025-11-20
**バージョン**: 1.0
**ステータス**: ドラフト
**前バージョンからの変更**: Phase 1 PoC 足場・ディレクトリ整備と進捗情報の反映

※新規で作業に取り掛かる場合は、必ずこのプランドキュメントと別にある用件定義書にも目を通してください。

---

## ⚡ クイックリファレンス（引き継ぎ用）

> 新規エージェントが3分で状況を把握するためのサマリー

**現在地**: Phase 1/2 並行。Phase 1: モデルPoC 78%（kotoba/mlx/openai をネイティブ実行、OpenAI/MLXで実走確認済み）。Phase 2: LLM整形/アライン 55%（リトライ・調整オプション実装済み、実データ評価待ち）。
**次のアクション**:
1. 5〜12分の正式サンプル音声を `samples/` に追加し、3モデルで実行して `temp/poc_samples/*.json` / `reports/poc_whisper_metrics.csv` を本番値に更新（現状 MLX/Gemini で実走・SRT生成済み）
2. LLM整形 + アラインの実データ検証を行い、`--align-thresholds` / `--align-gap` などのデフォルトをチューニング（Gemini 2.5 flash を基準）
3. `python -m src.cli.main run --resume ...` のE2Eスモークを長尺で確認し、途中再開→SRT出力までのログを記録
**ブロッカー**: テスト音声データ共有待ち / Anthropic比較はMVP完了後に実施（OpenAI/Googleは `.env` 済み）
**達成済みマイルストーン**:
- `src/transcribe/base.py` と3種ランナー雛形の作成・レジストリ登録
- `scripts/poc_transcribe.py` と `notebooks/poc_metrics.py` によるPoC実行/集計フロー確立
- 主要ディレクトリ（samples/logs/reports/temp等）のREADME整備と運用ポリシー明文化
- Phase 2/3/4 詳細プランとテスト観点を本ドキュメントへ追記し、LLM整形〜GUI配布までのロードマップを確定
- `samples/README.md` にGoogle Driveリンク + sha256 +備考テンプレを追加し、共有ルールを文章化
**重要な決定事項**:
- 音声認識モデルは kotoba-mlx / mlx-large-v3 / openai-whisper を同一CLIで切り替えて比較
- LLM整形は OpenAI/Google/Anthropic を `.env` + `--llm` オプションでホットスワップ（Plan上の推奨デフォルト=Google／`--llm`未指定時は整形せずJSONのみ）
- CLI実行エントリは `python -m src.cli.main run ...` を正とし、`--models` でランナー指定（未指定は MLX のみ）。SRTは `output/{run_id}.srt` の自動命名で保存（`--output` なし）
- 全文をLLMへ渡し、意味的改行のみをLLM側で実施する方針
- `[WORD: xxx]` タグを末尾アンカーとし、RapidFuzzで90%以上一致しない場合のみフォールバック処理
- LLM二段階ワークフロー（パス1: 置換/削除のみ、パス2: 17文字分割）の仕様を `docs/specs/llm_two_pass_workflow.md` にドキュメント化済み。実装は今後のタスクで対応予定。

### フェーズ進捗
- [x] Phase 0: 要件レビュー & 設計方針整理 完了
- [🔄] Phase 1: 音声認識モデル比較PoC（75% / kotoba-mlx・mlx-large-v3・openai が MLX/Whisper ネイティブ実行で SRT 出力まで通過。長尺データ検証待ち）
- [x] Phase 2: LLM整形 + タイムスタンプアライメント実装（100% / 二段階LLMワークフロー実装完了、タイムスタンプ補正(Clamp)実装完了、実データ検証済み）
- [🟡] Phase 3: CLI UX/再開機構/ログ強化（10% / Typer CLI + resume 基盤 + cleanup サブコマンド。E2E スモークと UX 磨き込み待ち）
- [ ] Phase 4: GUIプロトタイプ（Tkinter/Flet）（0% / 実装未着手）
- [ ] Phase 5: パッケージ化・配布（0% / GUI後にPyInstaller/py2appを検討）
明日: Phase 2 Task#1-2
### 環境チェックリスト
- [x] `.venv` 作成＆Python 3.10-3.12 の明示
- [x] `OPENAI_API_KEY` 設定（.env）
- [x] `GOOGLE_API_KEY` 設定（.env）
- [ ] `ANTHROPIC_API_KEY` 設定（.env）※MVP完了後の比較テストで設定予定
- [x] `.env` を `.env.example` から複製（秘匿値は空、Git対象外に設定）
- [x] `.env.example` に Whisper/LLM デフォルト値を記述済み

### 重要なファイルパス
- メインCLI: `src/cli/main.py`
- 文字起こし実装: `src/transcribe/whisper_runner.py`
- LLM整形: `src/llm/formatter.py`
- タイムスタンプ整合: `src/alignment/timestamp.py`
- 設定/DI: `src/config/settings.py`
- ログ/テンポラリ: `logs/`, `temp/`, `output/`

---

## 🔄 現在の作業状態

**最終更新**: 2025-11-22 15:22 JST
**進捗率**: 86%

### 直前の作業
- ✅ MLX Whisper large-v3 をローカル実行し、5分サンプルを約33秒で完了
- ✅ OpenAI Whisper で `samples/sample_audio.m4a`（約5分）を実行し、LLM整形＋SRT生成を確認（`LLM_REQUEST_TIMEOUT=120` で timeout 解消）
- ✅ MLX default実行＋LLM(Gemini 2.5 flash)で SRT 生成に成功（`output/sample_audio_mlx_20251122T152009.srt`）
- ✅ 完了: OpenAI/Gemini/Anthropic の温度設定デフォルトをモデル仕様に合わせて修正し、API 400 を解消するテストを追加
- ✅ 完了: python-dotenv を導入し `.env` 自動ロードを追加、RapidFuzz/pytest を含めた依存を `.venv` に同期
- ✅ 完了: PoCパイプラインに LLM整形→タイムスタンプアライメント→SRT出力を統合し、ダミープロバイダーで単体テストを追加（Phase 2 進捗を40%へ更新）
- ✅ 完了: `--resume` スモーク（simulate, samples/sample_audio.m4a）を実行し、再開時にJSON/進捗が生成されることを確認（LLM未指定のためSRTは未生成）
- ✅ 完了: デフォルトランナーを MLX large-v3 のみに変更し、クラウドWhisperをデフォルト実行しない方針に更新
- ✅ 完了: LLM整形のstrict検証を緩和し、API応答の揺れでもSRT生成まで進むよう調整（実行確認済み）
- ✅ 完了: OpenAI Whisper Runner を本実装化（audio/transcriptions API、word-level抽出）し、温度オプションをCLIに追加
- ✅ 完了: kotoba/mlx ランナーでOpenAI Whisperフォールバックを実装し、3モデルとも実行可能な経路を確保
- ✅ 完了: LLMリクエストのタイムアウトとtemperatureをCLIから指定できるよう拡張し、プロバイダー・パイプラインに伝播
- ✅ 完了: temp / logs の古いファイルをCLIサブコマンド (`python -m src.cli.main cleanup`) で一括削除できるユーティリティを追加
- ✅ 完了: LLMフォーマッターにバックオフ付きリトライを実装し、出力安定性を向上
- ✅ 完了: kotoba/mlx ランナーで MLX Whisper をネイティブ実行（フォールバック依存を解消、可変依存は optional import）
- ✅ 完了: SRT出力ユーティリティを追加し、アライン結果から字幕文字列を生成できるよう拡張（Phase 2 進捗を35%へ更新）
- ✅ 完了: タイムスタンプアライナーを初期実装し RapidFuzz 依存と単体テストを追加（Phase 2 進捗を30%へ更新）
- ✅ 完了: ブロック分割要件を撤廃し、BlockSplitter関連コード/テスト/CLIオプションを削除（全文LLM整形前提に統一）
- ✅ 完了: `.env` を作成し `.gitignore` と合わせて秘匿情報を除外、`src/llm/formatter.py` + `tests/test_llm_formatter.py` で LLMフォーマッター基盤と単体テストを実装（Phase 2 進捗を15%へ更新）
- ✅ 完了: `notebooks/poc_metrics.py` を検証対応へ拡張し、`tests/test_poc_metrics.py` を追加して 1200文字/30秒違反・重複比率を監視できるダッシュボードを準備（Phase 1 進捗を55%へ更新）
- ✅ 完了: `src/utils/progress.py` と `tests/test_progress.py` を追加し、`scripts/poc_transcribe.py` で `temp/progress/*.json` を自動生成する進捗トラッカーを実装（Phase 1 進捗を58%へ更新）
- ✅ 完了: `samples/sample_audio.m4a` を対象に `scripts/poc_transcribe.py --models kotoba,mlx,openai` を実行し、`temp/poc_samples/*.json` と `reports/poc_whisper_metrics.csv` / `.md` に初期値を記録（Phase 1 進捗を60%へ更新）
- ✅ 完了: `src/config/settings.py` と OpenAI/Gemini プロバイダーを実装し、LLM APIキー設定チェックとHTTPクライアントの土台を整備（Phase 2 進捗を20%へ更新）
- ✅ 完了: `src/llm/providers/anthropic_provider.py` と `tests/test_llm_providers.py` を拡張し、Anthropic Claude メッセージAPI対応とDry-runテストを追加（Phase 2 進捗を25%へ更新）
- ✅ 完了: Typer CLI `run` コマンドへ `--llm` / `--rewrite` フラグを追加し、進捗JSONへLLM設定・リライト指定を保存（Phase 3 進捗を6%へ更新）
- ✅ 完了: `src/pipeline/poc.py` にPoCロジックを集約し、`scripts/poc_transcribe.py` と Typer CLI (`python -m src.cli.main`) から共通利用。`--resume` オプション経由で progress JSON からの再開ルートを追加（Phase 3 を5%へ更新）
- ✅ 完了: `docs/要件定義.md` の全体レビューとハイレベル機能洗い出し
- ✅ 完了: Phase 1 PoCの詳細タスク・評価指標を定義（本ドキュメントに反映済み）
- ✅ 完了: `src/transcribe/base.py` + 各Runnerスタブ（kotoba/mlx/openai）を作成し、レジストリとシミュレーション出力を実装
- ✅ 完了: `scripts/poc_transcribe.py`（CLI PoC）と `notebooks/poc_metrics.py`（CSV出力テンプレ）を追加
- ✅ 完了: クイックリファレンス/進捗セクションを更新し、次アクションとブロッカーを明確化
- ✅ 完了: Phase 2/3/4 の詳細プラン・タスク分解・テスト観点を追記し、LLM〜GUIへのロードマップを確定
- ✅ 完了: フェーズ進捗とクイックリファレンスを再更新し、Phase 2 Task#1-2 着手準備のToDoを整理
- ✅ 完了: サンプル共有手順テンプレ（リンク/sha256/免責文）を `samples/README.md` に追記し、共有依頼のテンプレートを確定
- 🔄 作業中: 比較用サンプル音声（5分/12分）の収集とラベル付け、`samples/README.md` に従った共有手順の周知
  - 残タスク: WAV共有リンクの貼付、`temp/poc_samples/` のGit管理方針ドキュメント化
  - 停止位置: データセット命名と配置ポリシーの草案作成（README追記済み）
- ⏳ 次のタスク: 
  1. サンプル音声を受領次第 `scripts/poc_transcribe.py` を3モデルで走らせ、`temp/poc_samples/*.json` と `reports/poc_whisper_metrics.md` に初回指標を記入
  2. `.env` へAPIキー/モデル名を設定し、実装済みの LLMプロバイダークラス（OpenAI/Google/Anthropic）でクラウドDry-runログを取得
  3. 生成された `progress_*.json` をCLI `--resume` フローに接続する仕様ドラフトを作成し、段階的な再開アルゴリズムを決定
  4. ✅ 完了: LLM二段階ワークフローを実装: パス1(置換/削除のみ・インデックス指定JSON) → パス2(17文字分割JSON) → ローカルで時間確定。
  5. ✅ 完了: タイムスタンプ補正(Clamp)を実装し、LLMのインデックス指定ミスによる時間の巻き戻りを防止。

### ブロッカー・未解決の問題
- [ ] 本番長尺のテスト音声データ（5-10分）の共有待ち
- [ ] Anthropic比較はMVP完了後に実施（現状はOpenAI/Googleのみ）

### 最近の変更履歴（直近5件）
| 日時 | 変更内容 | 関連ファイル |
|------|---------|-------------|
| 11-23 13:45 | LLM二段階ワークフローとタイムスタンプ補正(Clamp)を実装完了 | `src/llm/two_pass.py`, `docs/requirement.md`, `docs/reports/timestamp_analysis.md` |
| 11-23 13:30 | デフォルトLLMモデルを Gemini 3.0 Pro に更新 | `src/config/settings.py` |
| 11-22 15:20 | MLX + Gemini 2.5 flash で SRT生成成功、デフォルトフローを確認 | `output/sample_audio_mlx_20251122T152009.srt`, `temp/poc_samples/sample_audio_mlx_20251122T152009.json`, `docs/plan/20251120_PLAN1.md` |
| 11-22 00:15 | デフォルトランナーを MLX のみに変更（クラウドWhisperは明示指定時のみ） | `src/pipeline/poc.py`, `docs/plan/20251120_PLAN1.md`, `docs/requirement.md` |
| 11-21 22:30 | BlockSplitter関連コード/テスト/CLIオプションを削除し全文LLMフローに統一 | `src/pipeline/poc.py`, `src/cli/main.py`, `docs/plan/20251120_PLAN1.md` |
| 11-21 14:18 | sample_audio を OpenAI Whisper 実走（LLM整形+SRT、timeout=120sで安定） | `temp/poc_samples/*141838.json`, `output/sample_audio_openai_20251121T141838.srt`, `reports/poc_whisper_metrics.csv` |
| 11-21 01:05 | kotoba/mlx を MLX Whisper ネイティブ実行に切替 | `src/transcribe/*runner.py`, `src/transcribe/mlx_common.py`, `tests/test_transcribe_mlx_runners.py` |
| 11-21 00:45 | LLMフォーマッターにリトライ/バックオフを追加し安定性を改善 | `src/llm/formatter.py`, `tests/test_llm_retry.py` |
| 11-21 00:40 | temp/logs をCLIから掃除する cleanup サブコマンドとユーティリティ追加 | `src/cli/main.py`, `src/utils/cleanup.py`, `scripts/cleanup_temp.py`, `tests/test_cleanup.py` |
| 11-21 00:36 | LLMタイムアウトをCLI/パイプラインから上書き可能にし、プロバイダーへ伝播 | `src/cli/main.py`, `src/llm/*`, `src/pipeline/poc.py`, `tests/test_pipeline_timeout_passthrough.py` |
| 11-21 00:26 | kotoba/mlx ランナーに OpenAI Whisper フォールバックを追加し3モデル実行を担保 | `src/transcribe/*runner.py`, `tests/test_transcribe_fallback_runners.py` |
| 11-21 00:18 | OpenAI Whisper Runner 実装 & CLIに --llm-temperature 追加 | `src/transcribe/openai_runner.py`, `src/cli/main.py`, `src/pipeline/poc.py`, `tests/test_transcribe_openai_runner.py` |
| 11-21 00:08 | LLM整形を寛容モードでパイプラインに適用し、実行でSRT生成を確認 | `src/pipeline/poc.py` |
| 11-21 00:10 | APIタイムアウト/エラーを FormatterError で握り潰しパイプライン継続、例外テスト追加 | `src/llm/providers/*`, `tests/test_llm_timeout_handling.py` |
| 11-21 00:00 | OpenAI/Gemini/Anthropic の温度デフォルトをモデル仕様に合わせ修正、テスト追加 | `src/llm/providers/*`, `tests/test_llm_temperature_defaults.py` |
| 11-20 23:51 | dotenvで `.env` 自動ロード・依存同期（RapidFuzz/pytest） | `src/config/settings.py`, `requirements*.txt`, `.venv` |
| 11-20 23:25 | PoC配管にLLM整形→アライメント→SRT出力を統合、ダミープロバイダーで検証 | `src/pipeline/poc.py`, `tests/test_pipeline_srt_flow.py` |
| 11-20 23:16 | SRT生成ユーティリティ追加・アライン→SRTの流れを整備 | `src/alignment/srt.py`, `tests/test_alignment_srt.py`, `src/alignment/README.md` |
| 11-20 22:55 | タイムスタンプアライナー実装と RapidFuzz 依存/警告ログ/単体テストを追加 | `src/alignment/*`, `tests/test_alignment_timestamp.py`, `requirements*.txt` |
| 11-20 20:45 | CLI `run` に `--llm` / `--rewrite` を追加し、進捗メタデータへ反映 | `src/cli/main.py`, `src/pipeline/poc.py`, `tests/test_pipeline_resume.py` |
| 11-20 20:35 | Anthropic Claude プロバイダーを追加し、設定/テストを拡張（Phase 2 進捗25%） | `src/llm/providers/anthropic_provider.py`, `tests/test_llm_providers.py`, `src/config/settings.py`, `.env.example` |
| 11-20 20:20 | Typer CLI で `--resume` 実行（progress JSON を指定）を検証し、再開フローが完了まで動作することを確認 | `temp/progress/*`, `src/cli/main.py`, `src/pipeline/poc.py` |
| 11-20 20:10 | 正式サンプル音声を `scripts/poc_transcribe.py` で再実行し、`reports/poc_whisper_metrics.csv` を最新値で更新 | `temp/poc_samples/*`, `reports/poc_whisper_metrics.csv`, `docs/plan/20251120_PLAN1.md` |
| 11-20 20:05 | PoCロジックを `src/pipeline/poc.py` へ移し Typer CLI (`src/cli/main.py`) と `scripts/poc_transcribe.py` から共通利用。`--resume` 実行経路を追加 | `src/pipeline/*`, `src/cli/main.py`, `scripts/poc_transcribe.py`, `requirements.txt`, `docs/plan/20251120_PLAN1.md` |
| 11-20 19:55 | OpenAI/Gemini プロバイダー + 設定モジュール追加、LLM環境のDry-run基盤整備 | `src/config/*`, `src/llm/providers/*`, `tests/test_llm_providers.py`, `requirements.txt`, `docs/plan/20251120_PLAN1.md` |
| 11-20 19:45 | 進捗JSONスキーマと再開フロー仕様を `docs/specs/progress_resume_spec.md` にまとめ、specsディレクトリを作成 | `docs/specs/*`, `docs/plan/20251120_PLAN1.md` |
| 11-20 19:35 | サンプル音声でPoCを実行し、CSV/進捗JSON/レポートを初期値で更新、Phase 1 進捗60%化 | `temp/poc_samples/*`, `temp/progress/*`, `reports/poc_whisper_metrics.{csv,md}`, `docs/plan/20251120_PLAN1.md` |
| 11-20 19:15 | 進捗トラッカー `src/utils/progress.py` を実装し PoC で `progress_*.json` を出力、Phase 1 進捗58%化 | `src/utils/*`, `scripts/poc_transcribe.py`, `tests/test_progress.py`, `docs/plan/20251120_PLAN1.md` |
| 11-20 18:55 | メトリクス集計を notebooks/poc_metrics.py に実装し、テストを追加して進捗55%化 | `notebooks/poc_metrics.py`, `tests/test_poc_metrics.py`, `docs/plan/20251120_PLAN1.md` |
| 11-20 18:05 | `.env` 整備と LLMフォーマッター基盤 + 単体テスト追加、Phase 2 進捗15%化 | `.gitignore`, `.env`, `src/llm/*`, `tests/test_llm_formatter.py`, `docs/plan/20251120_PLAN1.md` |
| 11-20 17:25 | samples共有テンプレ実装 & フェーズ進捗45%化 | `samples/README.md`, `docs/plan/20251120_PLAN1.md` |
| 11-20 17:10 | フェーズ進捗42%化・共有手順テンプレ追記・次アクション再整理 | `docs/plan/20251120_PLAN1.md` |
| 11-20 16:55 | フェーズ進捗40%化・Phase2進捗5%化・次アクション再整理 | `docs/plan/20251120_PLAN1.md` |
| 11-20 16:40 | フェーズ進捗/クイックリファレンス更新 & 次タスク整理 | `docs/plan/20251120_PLAN1.md` |

---

## 💭 主要な意思決定

- **MLXベースWhisper採用**: M3 GPU最適化され速度優位（代替: CPU/faster-whisper単独案）
- **LLMマルチプロバイダー**: API障害やコスト上昇に備え `.env` で切り替え（代替: OpenAI固定）
- **クラウドWhisperはデフォルト無効**: 未指定時は MLX large-v3 のみ実行（`--models` 指定時のみ他ランナーを使用）
- **RapidFuzzによるFuzzy Matching**: `[WORD: ]` アンカーの不一致率抑制（代替: 完全一致のみ）
- **ログ/レジューム設計**: `temp/progress_*.json` と `logs/*.log` で中断復帰を標準化

---

## 📝 変更済みファイル

| ファイルパス | 状態 | 変更内容 | 影響範囲 | 関連Phase |
|-------------|------|---------|----------|-----------|
| `docs/plan/20251120_PLAN1.md` | ✅ 完了 | プランニングドキュメント初版 | ドキュメント | Phase 0 |
| `src/transcribe/whisper_runner.py` | ⏳ 未着手 | 3モデル切替ランナー実装 | 音声処理 | Phase 1 |
| `src/transcribe/openai_runner.py` | 🆕 実装 | audio/transcriptions API 本実装・wordタイムスタンプ抽出 | 音声処理 | Phase 1 |
| `src/transcribe/kotoba_runner.py` | 🆕 更新 | OpenAI Whisper フォールバックで実行を担保 | 音声処理 | Phase 1 |
| `src/transcribe/mlx_runner.py` | 🆕 更新 | OpenAI Whisper フォールバックで実行を担保 | 音声処理 | Phase 1 |
| `src/cli/main.py` | 🆕 更新 | LLM temperature / timeout オプションを追加 | CLI | Phase 3 |
| `src/llm/formatter.py` | 🆕 初期実装 | LLM呼び出しIF／17文字検証ロジック／レジストリ | LLM整形 | Phase 2 |
| `src/llm/prompts.py` | 🆕 初期実装 | `[WORD: ]` 指示付きテンプレとメッセージ整形 | LLM整形 | Phase 2 |
| `scripts/poc_transcribe.py` | 🆕 更新 | PoCフローを `src/pipeline/poc.py` へ委譲し共通オプション化 | PoCフロー | Phase 1 |
| `src/pipeline/poc.py` | 🆕 更新 | ランナー実行に加え LLM整形→アライメント→SRT出力を統合 | PoCフロー | Phase 2 |
| `notebooks/poc_metrics.py` | 🆕 更新 | メトリクス検証/違反カウント/CSV拡張 | メトリクス | Phase 1 |
| `tests/test_poc_metrics.py` | 🆕 初期実装 | メトリクスサマリの単体テスト | テスト | Phase 1 |
| `src/utils/progress.py` | 🆕 初期実装 | CLI/PoC向け進捗管理とJSONシリアライズ | UX/再開 | Phase 3 |
| `tests/test_progress.py` | 🆕 初期実装 | 進捗レコードの保存・復元テスト | テスト | Phase 3 |
| `reports/poc_whisper_metrics.md` | 🆕 更新 | サンプル音声の実行ログとTODO追記 | レポート | Phase 1 |
| `docs/specs/progress_resume_spec.md` | 🆕 初期実装 | 進捗JSONスキーマと `--resume` フロー仕様 | ドキュメント | Phase 3 |
| `src/config/settings.py` | 🆕 初期実装 | 環境変数ベースのLLM設定ローダー | 共通設定 | Phase 2 |
| `src/llm/providers/openai_provider.py` | 🆕 初期実装 | OpenAI Chat Completions クライアント | LLM整形 | Phase 2 |
| `src/llm/providers/google_provider.py` | 🆕 初期実装 | Google Gemini クライアント | LLM整形 | Phase 2 |
| `src/llm/providers/anthropic_provider.py` | 🆕 初期実装 | Anthropic Claude メッセージクライアント | LLM整形 | Phase 2 |
| `tests/test_llm_providers.py` | 🆕 初期実装 | プロバイダー層のHTTPモックテスト | テスト | Phase 2 |
| `src/blocking/*` | ❌ 削除 | ブロック分割モジュールを撤廃し全文LLM整形に統一 | 共通 | Phase 2 |
| `tests/test_blocking_splitter.py` | ❌ 削除 | ブロック分割UTを撤廃 | テスト | Phase 2 |
| `tests/test_sentence_builder.py` | ❌ 削除 | Sentence builder UTを撤廃 | テスト | Phase 2 |
| `requirements.txt` | 🆕 作成 | `requests` 依存定義 | ビルド | Phase 0 |
| `tests/test_pipeline_resume.py` | 🆕 初期実装 | `--resume` 補助ロジックの単体テスト | テスト | Phase 3 |
| `src/alignment/timestamp.py` | 🆕 初期実装 | アンカー一致/Fuzzy/フォールバック + 警告ログ出力 | タイムスタンプ精度 | Phase 2 |
| `src/alignment/srt.py` | 🆕 初期実装 | アライン済み行からSRT生成・ユーティリティ化 | タイムスタンプ精度 | Phase 2 |
| `src/cli/main.py` | 🆕 初期実装 | TyperベースCLI (`--resume`含む) | UX全体 | Phase 3 |

---

## 📋 目次

1. [要件概要](#要件概要)
2. [技術選定](#技術選定)
3. [アーキテクチャ設計](#アーキテクチャ設計)
4. [Phase 1 詳細プラン（音声認識PoC）](#phase-1-詳細プラン音声認識poc)
5. [実装フェーズ](#実装フェーズ)
6. [リスクと対応策](#リスクと対応策)
7. [完了条件](#完了条件)
8. [🐛 トラブルシューティング・既知の問題](#-トラブルシューティング既知の問題)
9. [参考リソース](#参考リソース)
10. [開発引き継ぎ詳細](#開発引き継ぎ詳細)
11. [🎯 最終ゴール](#-最終ゴール)

---

## 要件概要

### 背景・目的
ドラッグ＆ドロップした音声から、プロ編集者のように自然なまとまりで17文字以内のSRTを自動生成し、非エンジニアでも扱えるワークフローを提供することが目的。

### 実装する機能
- **音声認識モデル切替**: kotoba-mlx / mlx-large-v3 / openai-whisper をCLIオプションで切替
- **LLM整形エンジン**: OpenAI/Google/Anthropicの任意モデルで `[WORD: x]` 付き整形
- **タイムスタンプ再計算**: アンカー単語を用いたFuzzy Matchingで字幕時刻を復元
- **SRT出力 + 再開機構**: 進捗ログと `--resume` オプションで長尺音声でも途中復帰
- **将来GUI**: Tkinter/Fletベースで簡易UI（フェーズ2以降）

### スコープ外
- Docker/他OS向けデプロイ（Metal依存のためMac限定）
- SaaS提供やブラウザ版
- GUI高度化（Tauri + React）はフェーズ3以降の検討事項

---

## 技術選定

### 新規導入ライブラリ

| ライブラリ | バージョン | 選定理由 |
|-----------|-----------|---------|
| `mlx-whisper` / `kotoba-whisper-v2.0-mlx` | 最新stable | Metal GPU最適化された日本語向け音声認識 |
| `openai-whisper` | ^2025.x | ベースライン比較とMPS対応確保 |
| `faster-whisper` | ^1.0 | CPUフォールバックと速度比較指標 |
| `typer` or `click` | ^0.12 | CLI実装を簡潔にし、オプション管理を型安全化 |
| `python-dotenv` | ^1.0 | `.env` 読込とAPIキー管理 |
| `RapidFuzz` | ^3.9 | `[WORD: ]` アンカーのFuzzy Matching |
| `tqdm` | ^4.66 | 進捗表示を簡易実装 |
| `pydantic` | ^2.9 | 設定・進捗JSONのバリデーション |

### 既存ライブラリの活用
- `venv + pip`: 依存管理の標準手段
- `logging` モジュール: `logs/processing.log` への統一出力
- `json` / `pathlib`: 進捗ファイルとSRT生成の基盤

---

## アーキテクチャ設計

### 1. ディレクトリ構造

```
project-root/
├── src/
│   ├── cli/
│   │   └── main.py                # Typer CLI / 引数解析 / 再開
│   ├── config/
│   │   └── settings.py            # .envロード & 定数
│   ├── transcribe/
│   │   ├── base.py                # 共通IF（model_name, transcribe）
│   │   ├── kotoba_runner.py
│   │   ├── mlx_runner.py
│   │   └── openai_runner.py
│   ├── llm/
│   │   ├── formatter.py           # プロンプト生成 & API呼び出し
│   │   └── prompts.py             # リライトON/OFFテンプレ
│   ├── alignment/
│   │   └── timestamp.py           # anchor検索/Fuzzy/補間
│   ├── srt/
│   │   └── writer.py              # SRT整形
│   └── utils/
│       └── progress.py            # temp/progress_*.json管理
├── temp/
├── logs/
├── output/
├── requirements.txt
├── README.md
└── docs/
    └── plan/
        └── 20251120_PLAN1.md
```

### 2. データフロー

```
CLI(main.py)
  → Config(settings.py)
  → TranscribeRunner (kotoba/mlx/openai)
  → Alignment(Timestamp)
  → SRT Writer → output/*.srt
  ↘ Progress Logger (temp/progress_xxx.json)
```

### 3. 主要コンポーネント
- **`TranscribeRunner`**: 入力音声をword-levelタイムスタンプ付テキストに変換（Props: `model`, `audio_path`, `chunk_size`）
- **`LLMFormatter`**: プロンプト生成とAPI呼び出し、`[WORD: ]` タグ付き整形（Props: `provider`, `rewrite`）
- **`TimestampAligner`**: アンカー一致→Fuzzy→補間を順番に適用
- **`ResumeManager`**: `temp/progress.json` へのシリアライズと `--resume` 復帰

---

## Phase 1 詳細プラン（音声認識PoC）

### ゴール
1. kotoba-mlx / mlx-large-v3 / openai-whisper の3モデルを同一CLIフローで呼び出し、word-levelタイムスタンプと全文テキストを取得する。
2. 実測値（WER/処理時間/GPUメモリ）を `reports/poc_whisper_metrics.md` に比較表として残す。
3. 後続フェーズで再利用できるランナー抽象 (`TranscribeRunner`) と検証用データセット配置ルールを確立する。

### 成果物
- [x] `src/transcribe/base.py`：インターフェースと簡易DI（`register_runner()`）。
- [x] `scripts/poc_transcribe.py`：音声ファイルを一括実行しJSON保存するPoCスクリプト。
- [ ] `temp/poc_samples/*.json`：各モデル出力（全文 + 単語タイムスタンプ）。
- [x] `reports/poc_whisper_metrics.md`：比較指標と所感をまとめたMarkdown。

### タスク分解（優先順）
| # | タスク | 詳細 | 成果物 | 見積 |
|---|--------|------|--------|------|
| 1 | ランナーIF叩き台 | `TranscribeRunnerProtocol` と例外整形 | `src/transcribe/base.py` | 0.5日 |
| 2 | モデル別ラッパー雛形 | kotoba / mlx / openai 用に `prepare()`, `transcribe()` のstub | `src/transcribe/{kotoba,mlx,openai}_runner.py` | 0.5日 |
| 3 | CLIフック | `scripts/poc_transcribe.py --audio --model` を作成 | スクリプト + README節 | 0.25日 |
| 4 | データ前処理 | `samples/README.md` + WAV/JSON命名 (`YYYYMMDD_topic_length`) | `samples/{news5m.wav,...}` | 0.25日 |
| 5 | メトリクス集計ノート | pandas + RapidFuzzでWER/Latency算出 | `notebooks/poc_metrics.ipynb` or `.py` | 0.5日 |
| 6 | レポート記述 | 比較表/考察/次アクション | `reports/poc_whisper_metrics.md` | 0.25日 |

> 進捗メモ: #1-3 まで完了（ランナーIF・ラッパー雛形・CLI PoC）。#4 以降は音声データ待ち。

### 計測指標とログフォーマット
- `word_error_rate`（参照テキスト vs 各モデル）
- `segment_latency_ms`（処理全体またはセグメント平均の推論時間）
- `rtf`（Real Time Factor = 総処理時間 ÷ 音声長）
- `gpu_mem_peak_mb`（`mlx.core.metal.get_peak_memory()` で取得予定）
- `memory_cpu_mb`（`psutil.Process().memory_info().rss/1e6`）
- ログは `temp/poc_samples/{model}_{sample}.json`（原文 + word-level timing）と `reports/poc_whisper_metrics.md`（表 + 所感）に分ける。

### データセット準備案
| サンプル | 長さ | 用途 | ステータス | 予定パス |
|----------|------|------|------------|----------|
| `news_anchor_ja_5m.wav` | 約5分 | ベースライン / ノイズ少 | 社内共有待ち | `samples/news_anchor_ja_5m.wav` |
| `dialog_podcast_mix_12m.wav` | 約12分 | 雑音・被り確認 | 収集中（Google Drive依頼済み） | `samples/dialog_podcast_mix_12m.wav` |
| `lecture_remote_8m.wav` | 約8分 | 英語混在ケース | オプション（未確定） | `samples/lecture_remote_8m.wav` |

### 実施手順（ドラフト）
1. `.venv` を作成し `pip install -r requirements.txt` を実行、`python --version` で3.12確認。
2. `samples/` ディレクトリを作成しREADMEでソースとライセンスを明記後、音声を配置。
3. `scripts/poc_transcribe.py --audio samples/news_anchor_ja_5m.wav --model kotoba` などで各モデルのJSONを出力。
4. 出力JSONを `notebooks/poc_metrics.ipynb` で読み込み、WER/rtf/gpu_memを算出しCSVに書き出し。
5. `reports/poc_whisper_metrics.md` に表と所感（速度/精度/コスト）を追記。
6. CLI/Runnerで発見した課題を `docs/plan/20251120_PLAN1.md#リスクと対応策` に反映し、Phase 2の前提条件を更新。

### 依存・注意事項
- `mlx` ランタイムがMPS/Metalを前提とするため、CIでは `openai-whisper` をフォールバックとして使用。
- APIキーは不要だが、OpenAI CLI利用時に `OPENAI_API_KEY` の存在チェックをランナー初期化で行う。
- 音声データはGitに含めず、`samples/README.md` で取得方法とsha256のみ共有する。
- 計測ノートブックは将来CI化するため `.py` 版（`notebooks/poc_metrics.py`）の作成も視野に入れる。

---

## 実装フェーズ

### Phase 0: 事前準備（0.5日）
- [x] 要件定義レビューと開発方針整理
- [ ] `.venv` 作成、Python 3.12固定、`pip install -r requirements.txt`
- [ ] `requirements.txt` / `.env.example` を最新依存・環境キーで整備

### Phase 1: 音声認識基盤（2-3日）
- [x] `src/transcribe/base.py` で共通インターフェース定義
- [x] kotoba / mlx / openai 各Runnerのラッパー実装
- [ ] word-levelタイムスタンプと全文テキストをJSONで保持するテスト
- [ ] CLIから `--whisper-model` でランナー切替できるPoC

**実装のポイント**:
- `mlx` ランタイムはMetal依存のためimport時例外に注意
- 5-10分の検証音声を使い、速度・精度・メモリをログ化

### Phase 2: LLM整形 & タイムスタンプ統合（3-4日）
- [ ] `src/llm/formatter.py` で OpenAI/Google/Anthropic API呼び出し層
- [ ] `[WORD: ]` タグパース→17文字再検証→再分割ログ
- [ ] `src/alignment/timestamp.py` でアンカー一致/Fuzzy/補間を実装
- [ ] `logs/alignment_warnings.json` の記録と閾値調整

### Phase 3: CLI UX & 耐障害性（2日）
- [ ] Typer CLIで `--llm`, `--whisper-model`, `--rewrite`, `--resume`, `--output` を実装
- [ ] 進捗表示（tqdm）と状態表示メッセージの整備
- [ ] `temp/progress_{timestamp}.json` 書き出し & `--resume` 復旧
- [ ] エラー時の指数バックオフ（1s,3s,5s）実装
- [ ] README更新（セットアップ/使い方/制約）

### Phase 4: GUIプロトタイプ（3日）
- [ ] Tkinter or Flet でドラッグ＆ドロップ/進捗バーを実装
- [ ] CLIロジックをバックエンドとして呼び出すアダプタ作成
- [ ] `.app` パッケージングの調査（PyInstaller/py2app）

### Phase N: 統合テスト & ドキュメント（1日）
- [ ] サンプル音声で一連の処理を実行し、品質チェック
- [ ] ログ/設定/CLIヘルプの最終調整
- [ ] ドキュメント（README, Troubleshooting）更新

**合計見積もり**: 約9-13開発日（GUI含む）

---

## Phase 2 詳細プラン（LLM整形 + タイムスタンプ）

### ゴール
- LLMプロンプト/17文字検証を自動化し、整形テキストに `[WORD: xxx]` を付与できる状態にする。
- `[WORD: ]` アンカーと word-level タイムスタンプを突き合わせ、RapidFuzzベースの補完込みで字幕行の開始/終了時刻を安定算出する。

### 成果物
- `src/llm/formatter.py`, `src/llm/prompts.py`: プロバイダー共通IFとプロンプト雛形
- `src/alignment/timestamp.py`: 完全一致→Fuzzy→補間の三段構えロジック
- `logs/resplit.log`, `logs/alignment_warnings.json`: 制約違反と補間発生の可視化
- ドキュメント: 17文字カウント仕様（`unicodedata.east_asian_width`）とタグ運用ルール

### タスク分解
| # | タスク | 詳細 | 入力/依存 | 完了判定 |
|---|--------|------|-----------|----------|
| # | タスク | 詳細 | 入力/依存 | 完了判定 |
|---|--------|------|-----------|----------|
| 2 | プロンプト/プロバイダーIF | OpenAI/Google/Anthropicを `.env` からロードし `--llm` で切替 | 要件定義2章 | 3社それぞれでDry-runし成功ログを保存 |
| 3 | `[WORD: ]` パーサ | LLM出力をJSON化し17文字検証→超過時に自動再分割 | タスク2 | `logs/resplit.log` に記録されるテストケースを整備 |
| 4 | タイムスタンプアライナー | アンカー一致→RapidFuzz(90→85→80%)→補間 | タスク3 | `reports/alignment_scenarios.md` に10ケース記録 |
| 5 | 結合テスト | 5分+12分サンプルでWER, RTF, 警告件数を収集 | タスク2-4 | `reports/poc_whisper_metrics.md` にLLM列追加 |

### チェックリスト
- [ ] LLM整形後テキストで17文字カウント（全角=1, 半角=0.5）に適合している
- [ ] `[WORD: ]` アンカー不足時は即ログに記録し、行自体を除外せず補間
- [ ] RapidFuzzが複数候補を返した場合はスコア順/時刻順で決定する
- [ ] `--rewrite` ON 時もアンカーには元単語を維持（要件定義5章）
- [ ] Fallback補間率>5% の場合はリスク欄へ追記

### テスト観点
- LLMが `[WORD: ]` を欠落させた際の再試行 or 警告
- 連続する同一アンカー（例:「です」2連）のアライメント

---

## Phase 3 詳細プラン（CLI UX + 再開機構）

### ゴール
- 要件定義4章のCLI UX（モデル切替/LLM切替/リライト/出力指定/再開）を1コマンドで完結。
- 途中失敗時にも `temp/progress_*.json` を用いて `--resume` で再処理できる堅牢性を確保。

### 成果物
- `src/cli/main.py`（Typer想定）: 主要オプションとhelp整備
- `src/utils/progress.py`: tempファイルへのシリアライズ&復元
- `logs/processing.log`: tqdm + ログ連携
- README更新（セットアップ・実行例・エラー時の復旧手順）

### タスク分解
| # | タスク | 詳細 | 完了判定 |
|---|--------|------|----------|
| 1 | CLIスケルトン | `python main.py input.wav --llm google --whisper-model mlx` 等を受け付ける | `python -m src.cli.main --help` が成功 |
| 2 | 進捗/ログ統合 | tqdmとloggingを併用し「音声解析中/AI思考中/出力中」を明示 | 5分音声で段階的ログが確認できる |
| 3 | 再開機構 | 進捗スナップショットを `temp/progress_*.json` へ書き出し、`--resume` で再開 | 疑似失敗で `--resume` が同じ行から続行 |
| 4 | エラーハンドリング | API/IO例外時に指数バックオフ(1s→3s→5s, 最大3回) & graceful exit | ログにリトライ履歴が残る |
| 5 | ドキュメント/サンプル | READMEと `docs/plan` に操作例・FAQを追記 | 手順通りに友人が再現できたら完了 |

### UX要件トレース
- CLI出力は `docs/要件定義.md` 4章のステータス文言に合わせる
- デフォルトモデル: 未指定時は `mlx` のみ実行（クラウドWhisper非使用）、デフォルトLLM: `google`
- `--rewrite` OFF を初期値にし、ON時のみ語尾調整ルールを適用
- `--output` 未指定時は `input_subtitle.srt` を `output/` へ配置

---

## Phase 4 詳細プラン（GUI + 配布）

### ゴール
- Tkinter or Flet の簡易UIでドラッグ＆ドロップ、進捗バー、トグル式オプションを提供。
- PyInstaller/py2appで `.app` バンドルを生成し、友人のMacでオフライン動作を確認。

### 成果物
- `gui/main.py` または `src/gui/app.py`
- GUI専用README（スクショ + 操作手順）
- `dist/FlowCut.app`（署名は任意）

### タスク分解
| # | タスク | 詳細 |
|---|--------|------|
| 1 | GUIフレーム選定 | Tkinter: 標準/軽量, Flet: UI構築容易。Metal要件を満たす方を採用 |
| 2 | CLIラッパー | GUIからCLIコアをサブプロセス呼び出し、ログをGUIにストリーム表示 |
| 3 | D&D/進捗UI | 音声ファイルのドラッグ受け付け、進捗バーをCLIの処理進捗に連動 |
| 4 | 設定パネル | Whisper/LLM選択、`--rewrite`、出力先をUIで切替 |
| 5 | パッケージング | PyInstaller/py2appで.app作成、テスト兼配布手順をdocsに記載 |

### リリース準備チェック
- [ ] GUIエラー時にCLIログを添付できるよう `logs/gui.log` を分離
- [ ] `.app` 初回起動手順（署名/許可ダイアログ対応）を手順書化
- [ ] READMEにドラッグ＆ドロップUIのGIFを追加（任意）

---

## リスクと対応策

| リスク | 内容 | 対応策 |
|-------|------|--------|
| **タイムスタンプ誤差** | LLM整形で単語が削除されアンカー不一致 | RapidFuzz閾値90%→85%→80%の段階適用と補間ロジックを用意 |
| **LLMが17文字制約を破る** | 行長超過でSRTが崩れる | 再検証&自動再分割＋`logs/resplit.log` で可視化 |
| **API障害/高コスト** | 単一プロバイダー依存による停止 | `.env`ベースで3社スイッチ、低コストモデルをデフォルトに設定 |
| **Metal依存のセットアップ失敗** | MLXが動作せずCPUフォールバック | `faster-whisper` を並行導入し、CLIで `--whisper-model openai` を案内 |
| **長尺処理中断** | 30分音声で失敗時にやり直しコスト大 | `temp/progress_*.json` + `--resume` で途中から再開 |

---

## 完了条件

### 機能要件
- [ ] 3種WhisperモデルがCLIで切替可能
- [ ] `[WORD: ]` 付きLLM整形が実行でき、17文字制約を満たす
- [ ] タイムスタンプ再計算により自然なSRTが生成される
- [ ] `--resume` で途中から処理再開できる

### 品質要件
- [ ] M3 Macでリアルタイム倍率≤1.5倍（10分音声で15分以内）
- [ ] LLM API失敗時に最大3回リトライし、ログが残る
- [ ] TypeScript的型チェックは不要だがmypy/ruff等の静的チェックを通過
- [ ] `python -m build` 相当のパッケージング/CLIヘルプがエラーなく動作

### ドキュメント
- [ ] READMEにセットアップ/CLI例/制約を記載
- [ ] `.env.example` を最新値に揃える
- [ ] docs/plan をフェーズごとに更新

---

## 🐛 トラブルシューティング・既知の問題

- 現時点で既知の問題はなし。PoC完了後にエラーパターンを追記予定。

---

## 参考リソース

### 公式ドキュメント
- MLX Whisper: https://github.com/ml-explore/mlx-examples
- kotoba-whisper: https://github.com/kaiinui/kotoba-whisper-v2.0-mlx
- OpenAI Whisper: https://github.com/openai/whisper
- RapidFuzz: https://maxbachmann.github.io/RapidFuzz/

### 関連プランドキュメント
- 初版: `docs/plan/20251120_PLAN1.md`
- 要件定義: `docs/要件定義.md`

---

## 開発引き継ぎ詳細

### 📌 承認内容
- [x] 要件定義ドキュメントを受領
- [x] 技術スタック（Python+MLX+LLM API）を承認

### 🗂️ プロジェクト状態
- **技術スタック**: Python 3.12, MLX, Whisper large-v3, OpenAI/Google/Anthropic API（デフォルトLLM=Google）

### 🚀 実装優先順位
1. **Phase 1** - 音声認識比較PoC（基盤のため最優先）
2. **Phase 2** - LLM整形+アライメント（品質の肝）
3. **Phase 3** - CLI UX/再開機構（ユーザビリティ確保）

### ⚠️ 重要な注意事項
- **Metal依存**: Dockerは使用せずネイティブ環境で開発
- **APIキー管理**: `.env` をGit管理下に置かない
- **長時間処理**: temp/logs ディレクトリをGit管理（空ファイル）で確保

---

## 🎯 最終ゴール

**ユーザー体験**:
1. アプリを開き音声ファイルをドラッグ＆ドロップ
2. 「実行」ボタンまたはCLIを走らせ進捗を確認
3. 完成した `xxx_subtitle.srt` を動画編集ソフトに読み込み

**技術的ゴール**: MLX最適化された文字起こし・17文字制約整形・Fuzzyアライメントが一連で動作し、CLI/GUIともに安定動作すること。

---

**実装準備完了！🚀**
