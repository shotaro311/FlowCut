# プランニングドキュメント：Flow Cut コアCLI実装

**作成日**: 2025-11-20
**バージョン**: 1.0
**ステータス**: ドラフト
**前バージョンからの変更**: 初版作成

---

## ⚡ クイックリファレンス（引き継ぎ用）

> 新規エージェントが3分で状況を把握するためのサマリー

**現在地**: Phase 0 - `docs/要件定義.md` 要件レビュー（100%完了）
**次のアクション**: kotoba/mlx/openai Whisper比較用のPoCスクリプトを作成し、word-levelタイムスタンプ品質を測定
**ブロッカー**: なし（APIキー未設定のためLLM呼び出し未検証）
**重要な決定事項**:
- 音声認識モデルは kotoba-mlx / mlx-large-v3 / openai-whisper を同一CLIで切り替えて比較
- LLM整形は OpenAI/Google/Anthropic を `.env` + `--llm` オプションでホットスワップ
- ブロック処理は 1200文字 or 30秒の条件で分割し、末尾1-2文の重複ウィンドウを採用
- `[WORD: xxx]` タグを末尾アンカーとし、RapidFuzzで90%以上一致しない場合のみフォールバック処理

### フェーズ進捗
- [x] Phase 0: 要件レビュー & 設計方針整理 完了
- [🔄] Phase 1: 音声認識モデル比較PoC（0%）← **今ここ**
- [ ] Phase 2: LLM整形 + タイムスタンプアライメント実装
- [ ] Phase 3: CLI UX/再開機構/ログ強化
- [ ] Phase 4: GUIプロトタイプ（Tkinter/Flet）

### 環境チェックリスト
- [ ] `.venv` 作成＆Python 3.10-3.12 の明示
- [ ] `OPENAI_API_KEY` 設定（.env）
- [ ] `GOOGLE_API_KEY` 設定（.env）
- [ ] `ANTHROPIC_API_KEY` 設定（.env）
- [x] `.env.example` に Whisper/LLM デフォルト値を記述済み

### 重要なファイルパス
- メインCLI: `src/cli/main.py`
- 文字起こし実装: `src/transcribe/whisper_runner.py`
- LLM整形: `src/llm/formatter.py`
- タイムスタンプ整合: `src/alignment/timestamp.py`
- 設定/DI: `src/config/settings.py`
- ログ/テンポラリ: `logs/`, `temp/`, `output/`

---

## 🔄 現在の作業状態

**最終更新**: 2025-11-20 15:24 JST
**進捗率**: 5%

### 直前の作業
- ✅ 完了: `docs/要件定義.md` の全体レビューとハイレベル機能洗い出し
- 🔄 作業中: PoC構成案の策定（進捗0% - 本ドキュメントで要件を整理）
  - 残タスク: ランナー層/CLI層の責務分割、比較用データセット決定
  - 停止位置: 設計ドキュメント作成中
- ⏳ 次のタスク: 音声認識PoCのスケルトン (`src/transcribe/*.py`) 作成

### ブロッカー・未解決の問題
- [ ] APIキー未設定のためLLM整形とクラウド課金の確認が未実施
- [ ] テスト音声データ（5-10分）の共有待ち

### 最近の変更履歴（直近5件）
| 日時 | 変更内容 | 関連ファイル |
|------|---------|-------------|
| 11-20 15:20 | 要件定義の全編読込 | `docs/要件定義.md` |
| 11-20 15:24 | プラン初版の骨子作成 | `docs/plan/20251120_PLAN1.md` |

---

## 💭 主要な意思決定

- **MLXベースWhisper採用**: M3 GPU最適化され速度優位（代替: CPU/faster-whisper単独案）
- **LLMマルチプロバイダー**: API障害やコスト上昇に備え `.env` で切り替え（代替: OpenAI固定）
- **ブロック重複方式**: 文脈維持のため末尾文の重複を許容（代替: 単純固定長分割）
- **RapidFuzzによるFuzzy Matching**: `[WORD: ]` アンカーの不一致率抑制（代替: 完全一致のみ）
- **ログ/レジューム設計**: `temp/progress_*.json` と `logs/*.log` で中断復帰を標準化

---

## 📝 変更済みファイル

| ファイルパス | 状態 | 変更内容 | 影響範囲 | 関連Phase |
|-------------|------|---------|----------|-----------|
| `docs/plan/20251120_PLAN1.md` | ✅ 完了 | プランニングドキュメント初版 | ドキュメント | Phase 0 |
| `src/transcribe/whisper_runner.py` | ⏳ 未着手 | 3モデル切替ランナー実装 | 音声処理 | Phase 1 |
| `src/llm/formatter.py` | ⏳ 未着手 | LLM呼び出し＆17文字検証 | LLM整形 | Phase 2 |
| `src/alignment/timestamp.py` | ⏳ 未着手 | アンカー一致＆補正ロジック | タイムスタンプ精度 | Phase 2 |
| `src/cli/main.py` | ⏳ 未着手 | Typer/ClickベースCLI & 再開処理 | UX全体 | Phase 3 |

---

## 📋 目次

1. [要件概要](#要件概要)
2. [技術選定](#技術選定)
3. [アーキテクチャ設計](#アーキテクチャ設計)
4. [実装フェーズ](#実装フェーズ)
5. [リスクと対応策](#リスクと対応策)
6. [完了条件](#完了条件)
7. [🐛 トラブルシューティング・既知の問題](#-トラブルシューティング既知の問題)
8. [参考リソース](#参考リソース)
9. [開発引き継ぎ詳細](#開発引き継ぎ詳細)
10. [🎯 最終ゴール](#-最終ゴール)

---

## 要件概要

### 背景・目的
ドラッグ＆ドロップした音声から、プロ編集者のように自然なまとまりで17文字以内のSRTを自動生成し、非エンジニアでも扱えるワークフローを提供することが目的。

### 実装する機能
- **音声認識モデル切替**: kotoba-mlx / mlx-large-v3 / openai-whisper をCLIオプションで切替
- **LLM整形エンジン**: OpenAI/Google/Anthropicの任意モデルで `[WORD: x]` 付き整形
- **タイムスタンプ再計算**: アンカー単語を用いたFuzzy Matchingで字幕時刻を復元
- **SRT出力 + 再開機構**: 進捗ログと `--resume` オプションで長尺音声でも途中復帰
- **将来GUI**: Tkinter/Fletベースで簡易UI（フェーズ2以降）

### スコープ外
- Docker/他OS向けデプロイ（Metal依存のためMac限定）
- SaaS提供やブラウザ版
- GUI高度化（Tauri + React）はフェーズ3以降の検討事項

---

## 技術選定

### 新規導入ライブラリ

| ライブラリ | バージョン | 選定理由 |
|-----------|-----------|---------|
| `mlx-whisper` / `kotoba-whisper-v2.0-mlx` | 最新stable | Metal GPU最適化された日本語向け音声認識 |
| `openai-whisper` | ^2025.x | ベースライン比較とMPS対応確保 |
| `faster-whisper` | ^1.0 | CPUフォールバックと速度比較指標 |
| `typer` or `click` | ^0.12 | CLI実装を簡潔にし、オプション管理を型安全化 |
| `python-dotenv` | ^1.0 | `.env` 読込とAPIキー管理 |
| `RapidFuzz` | ^3.9 | `[WORD: ]` アンカーのFuzzy Matching |
| `tqdm` | ^4.66 | 進捗表示を簡易実装 |
| `pydantic` | ^2.9 | 設定・進捗JSONのバリデーション |

### 既存ライブラリの活用
- `venv + pip`: 依存管理の標準手段
- `logging` モジュール: `logs/processing.log` への統一出力
- `json` / `pathlib`: 進捗ファイルとSRT生成の基盤

---

## アーキテクチャ設計

### 1. ディレクトリ構造

```
project-root/
├── src/
│   ├── cli/
│   │   └── main.py                # Typer CLI / 引数解析 / 再開
│   ├── config/
│   │   └── settings.py            # .envロード & 定数
│   ├── transcribe/
│   │   ├── base.py                # 共通IF（model_name, transcribe）
│   │   ├── kotoba_runner.py
│   │   ├── mlx_runner.py
│   │   └── openai_runner.py
│   ├── llm/
│   │   ├── formatter.py           # プロンプト生成 & API呼び出し
│   │   └── prompts.py             # リライトON/OFFテンプレ
│   ├── alignment/
│   │   └── timestamp.py           # anchor検索/Fuzzy/補間
│   ├── srt/
│   │   └── writer.py              # SRT整形
│   └── utils/
│       └── progress.py            # temp/progress_*.json管理
├── temp/
├── logs/
├── output/
├── requirements.txt
├── README.md
└── docs/
    └── plan/
        └── 20251120_PLAN1.md
```

### 2. データフロー

```
CLI(main.py)
  → Config(settings.py)
  → TranscribeRunner (kotoba/mlx/openai)
  → BlockSplitter + LLM Formatter
  → Alignment(Timestamp)
  → SRT Writer → output/*.srt
  ↘ Progress Logger (temp/progress_xxx.json)
```

### 3. 主要コンポーネント
- **`TranscribeRunner`**: 入力音声をword-levelタイムスタンプ付テキストに変換（Props: `model`, `audio_path`, `chunk_size`）
- **`BlockProcessor`**: 1200文字/30秒で分割し、重複ウィンドウ追加（Props: `max_chars`, `max_duration`）
- **`LLMFormatter`**: プロンプト生成とAPI呼び出し、`[WORD: ]` タグ付き整形（Props: `provider`, `rewrite`）
- **`TimestampAligner`**: アンカー一致→Fuzzy→補間を順番に適用
- **`ResumeManager`**: `temp/progress.json` へのシリアライズと `--resume` 復帰

---

## 実装フェーズ

### Phase 0: 事前準備（0.5日）
- [x] 要件定義レビューと開発方針整理
- [ ] `.venv` 作成、Python 3.12固定、`pip install -r requirements.txt`
- [ ] `requirements.txt` / `.env.example` を最新依存・環境キーで整備

### Phase 1: 音声認識基盤（2-3日）
- [ ] `src/transcribe/base.py` で共通インターフェース定義
- [ ] kotoba / mlx / openai 各Runnerのラッパー実装
- [ ] word-levelタイムスタンプと全文テキストをJSONで保持するテスト
- [ ] CLIから `--whisper-model` でランナー切替できるPoC

**実装のポイント**:
- `mlx` ランタイムはMetal依存のためimport時例外に注意
- 5-10分の検証音声を使い、速度・精度・メモリをログ化

### Phase 2: LLM整形 & タイムスタンプ統合（3-4日）
- [ ] `src/llm/formatter.py` で OpenAI/Google/Anthropic API呼び出し層
- [ ] ブロック分割（1200文字/30秒）と重複ウィンドウ実装
- [ ] `[WORD: ]` タグパース→17文字再検証→再分割ログ
- [ ] `src/alignment/timestamp.py` でアンカー一致/Fuzzy/補間を実装
- [ ] `logs/alignment_warnings.json` の記録と閾値調整

### Phase 3: CLI UX & 耐障害性（2日）
- [ ] Typer CLIで `--llm`, `--whisper-model`, `--rewrite`, `--resume`, `--output` を実装
- [ ] 進捗表示（tqdm）と状態表示メッセージの整備
- [ ] `temp/progress_{timestamp}.json` 書き出し & `--resume` 復旧
- [ ] エラー時の指数バックオフ（1s,3s,5s）実装
- [ ] README更新（セットアップ/使い方/制約）

### Phase 4: GUIプロトタイプ（3日）
- [ ] Tkinter or Flet でドラッグ＆ドロップ/進捗バーを実装
- [ ] CLIロジックをバックエンドとして呼び出すアダプタ作成
- [ ] `.app` パッケージングの調査（PyInstaller/py2app）

### Phase N: 統合テスト & ドキュメント（1日）
- [ ] サンプル音声で一連の処理を実行し、品質チェック
- [ ] ログ/設定/CLIヘルプの最終調整
- [ ] ドキュメント（README, Troubleshooting）更新

**合計見積もり**: 約9-13開発日（GUI含む）

---

## リスクと対応策

| リスク | 内容 | 対応策 |
|-------|------|--------|
| **タイムスタンプ誤差** | LLM整形で単語が削除されアンカー不一致 | RapidFuzz閾値90%→85%→80%の段階適用と補間ロジックを用意 |
| **LLMが17文字制約を破る** | 行長超過でSRTが崩れる | 再検証&自動再分割＋`logs/resplit.log` で可視化 |
| **API障害/高コスト** | 単一プロバイダー依存による停止 | `.env`ベースで3社スイッチ、低コストモデルをデフォルトに設定 |
| **Metal依存のセットアップ失敗** | MLXが動作せずCPUフォールバック | `faster-whisper` を並行導入し、CLIで `--whisper-model openai` を案内 |
| **長尺処理中断** | 30分音声で失敗時にやり直しコスト大 | `temp/progress_*.json` + `--resume` で途中ブロックから再開 |

---

## 完了条件

### 機能要件
- [ ] 3種WhisperモデルがCLIで切替可能
- [ ] `[WORD: ]` 付きLLM整形が実行でき、17文字制約を満たす
- [ ] タイムスタンプ再計算により自然なSRTが生成される
- [ ] `--resume` で途中から処理再開できる

### 品質要件
- [ ] M3 Macでリアルタイム倍率≤1.5倍（10分音声で15分以内）
- [ ] LLM API失敗時に最大3回リトライし、ログが残る
- [ ] TypeScript的型チェックは不要だがmypy/ruff等の静的チェックを通過
- [ ] `python -m build` 相当のパッケージング/CLIヘルプがエラーなく動作

### ドキュメント
- [ ] READMEにセットアップ/CLI例/制約を記載
- [ ] `.env.example` を最新値に揃える
- [ ] docs/plan をフェーズごとに更新

---

## 🐛 トラブルシューティング・既知の問題

- 現時点で既知の問題はなし。PoC完了後にエラーパターンを追記予定。

---

## 参考リソース

### 公式ドキュメント
- MLX Whisper: https://github.com/ml-explore/mlx-examples
- kotoba-whisper: https://github.com/kaiinui/kotoba-whisper-v2.0-mlx
- OpenAI Whisper: https://github.com/openai/whisper
- RapidFuzz: https://maxbachmann.github.io/RapidFuzz/

### 関連プランドキュメント
- 初版: `docs/plan/20251120_PLAN1.md`
- 要件定義: `docs/要件定義.md`

---

## 開発引き継ぎ詳細

### 📌 承認内容
- [x] 要件定義ドキュメントを受領
- [x] 技術スタック（Python+MLX+LLM API）を承認

### 🗂️ プロジェクト状態
- **技術スタック**: Python 3.12, MLX, Whisper large-v3, OpenAI/Google/Anthropic API
- **動作確認済み機能**: なし（これから実装）
- **既知の問題**: なし（PoC前）

### 🚀 実装優先順位
1. **Phase 1** - 音声認識比較PoC（基盤のため最優先）
2. **Phase 2** - LLM整形+アライメント（品質の肝）
3. **Phase 3** - CLI UX/再開機構（ユーザビリティ確保）

### ⚠️ 重要な注意事項
- **Metal依存**: Dockerは使用せずネイティブ環境で開発
- **APIキー管理**: `.env` をGit管理下に置かない
- **長時間処理**: temp/logs ディレクトリをGit管理（空ファイル）で確保

---

## 🎯 最終ゴール

**ユーザー体験**:
1. アプリを開き音声ファイルをドラッグ＆ドロップ
2. 「実行」ボタンまたはCLIを走らせ進捗を確認
3. 完成した `xxx_subtitle.srt` を動画編集ソフトに読み込み

**技術的ゴール**: MLX最適化された文字起こし・17文字制約整形・Fuzzyアライメントが一連で動作し、CLI/GUIともに安定動作すること。

---

**実装準備完了！🚀**
